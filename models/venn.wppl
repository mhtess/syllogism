// webppl venn.wppl --require webppl-csv

var foreach = function(lst, fn) {
  var foreach_ = function(i) {
    if (i < lst.length) {
      fn(lst[i]);
      foreach_(i + 1);
    }
  };
  foreach_(0);
};

var dataFrame = function(rawCSV) {
  return map(function(row) {
    return _.fromPairs(_.zip(rawCSV[0], row))
  }, rawCSV.slice(1))
}

var levels = function(df, label) {
  return _.uniq(_.map(df, label));
}

var displayDist = function(dist) {
  foreach(dist.support(), function(x) {
    // display(x + ", " + Math.exp(dist.score(x)))
    display(JSON.stringify(x) + ", " + Math.exp(dist.score(x)))
  })
}
var formatSentence = function(sentence) {

  var quantifier = (sentence.quantifier == "nvc") ? "NVC" :
  (sentence.quantifier == "some_not") ? "Some not" :
   sentence.quantifier.charAt(0).toUpperCase() + sentence.quantifier.slice(1)

  return quantifier == "NVC" ? "NVC;" : quantifier + ";" + sentence.p1 + ";" + sentence.p2
}
var formatQuantifier = function(q) {
  q.toLowerCase().split(' ').join("_")
}

var regions = Enumerate(function() {
  var A = flip(),
      B = flip(),
      C = flip()
  condition(A + B + C > 0)
  return {A, B, C}
}).support()
// display(JSON.stringify(all_regions))

// var regions = filter(function(x){ any(function(i){ i[1]}, _.toPairs(x)) }, all_regions)
// display(JSON.stringify(regions))

var distProbs = function(dist, supp) {
  return map(function(s) {
    return Math.exp(dist.score(s))
  }, supp)
}

var KL = function(p, q, supp) {
  var P = distProbs(p, supp),
      Q = distProbs(q, supp);
  var diverge = function(xp, xq) {
    return xp == 0 ? 0 : (xp * Math.log(xp / xq));
  };
  return sum(map2(diverge, P, Q));
};




// var venn_space = Enumerate(function(){
// 	map(function(region){ return {region: region, truth_val: flip() } }, regions)
// })


var parseVennDiagram = function(r) {
  map(function(i) {
    i[1] ? i[0] : ""
  }, _.toPairs(r)).join("")
}

var regionLabels = map(parseVennDiagram, regions)

var iidProbs = {
  // "": 0.5,
  "": 0.5,
  "A": 0.5,
  "B": 0.5,
  "C": 0.5,
  "AB": 0.5,
  "BC": 0.5,
  "AC": 0.5,
  "ABC": 0.5
}

var correlatedProbs = {
  "": 0.5,
  "A": 0.25,
  "B": 0.25,
  "C": 0.25,
  "AB": 0.50,
  "BC": 0.50,
  "AC": 0.50,
  "ABC": 0.75
}

var flatProbs = {
  "": 0.5,
  "A": 0.5,
  "B": 0.5,
  "C": 0.5,
  "AB": 0.5,
  "BC": 0.5,
  "AC": 0.5,
  "ABC": 0.5
}

var regionProbs = iidProbs;

var full_venn_space = Enumerate(function() {
  map(function(region) {
    var regionLabel = parseVennDiagram(region)
    var priorProb = regionProbs[regionLabel]
    return {
      region: region,
      priorProb: priorProb,
      truth_val: flip(priorProb),
      label: parseVennDiagram(region)
    }
  }, regions)
})

var venn_space = Infer({model: function(){
  var v = sample(full_venn_space)
  condition(any(function(x){ x.truth_val }, v))
  return v
}})


var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}


// SHOULD BE PAIRED WITH all()
// this DOES NOT includes an existential presupposition
var all_filter = function(x, y) {
  return function(r) {
    return r.region[x] ? // if its an A
      !r.region[y] ? // and not B
      !r.truth_val : // then it must be false
      true : // if it's an A and not a B, then it must be false
      true // if it's not an A, then it doesn't affect overall truthval (because we will use "all" to search that all are positive truth val)
  }
}

// SHOULD BE PAIRED WITH any()
// this includes an existential presupposition
var some_filter = function(x, y) {
  return function(r) {
    return r.region[x] ? // if its an A
      r.region[y] ? // and a B
      r.truth_val : // then this is the relevant region, and select its truthval
      false : // if it's an A and not a B, then it doesn't contribute a positive truth val (because we will use "any" to search for any positive truth val)
      false // if it's not an A, then it doesn't contribute a positive truth val (because we will use "any" to search for any positive truth val)
  }
}

var lexicon = {
  all: function(state, property1, property2) {
    return (all(all_filter(property1, property2), state) &&
      any(some_filter(property1, property2), state))
  },
  some: function(state, property1, property2) {
    return any(some_filter(property1, property2), state)
  },
  none: function(state, property1, property2) {
    return !(any(some_filter(property1, property2), state))
  },
  some_not: function(state, property1, property2) {
    return !(all(all_filter(property1, property2), state) &&
      any(some_filter(property1, property2), state))
    // return !(all(all_filter(property1, property2), state))
  },
  nvc: function(state, property1, property2) {
    return true
  }
}

var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}
var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}
// var listener0 = Enumerate(function(){
// 	var state = sample(venn_space)
// 	var predicate_filter = function(x, y){
// 		filter(function(r){return { region: }}, state)
// 	}
// 	condition(state.)
// })

var quantifiers = ["all", "some", "none", "some_not"]

// var conclusions = ["all", "some", "none", "some_not", "nvc"]

var conclusions = [{
    p1: "A",
    p2: "C",
    quantifier: "all"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "some"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "none"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "some_not"
  },
  {
    p1: "C",
    p2: "A",
    quantifier: "all"
  },
  {
    p1: "C",
    p2: "A",
    quantifier: "some_not"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "nvc"
  }
]

// var syllogisicSpace = Enumerate(function(){
//   { p1: }
// })

// var PremisePrior = cache(function(observed_premises){
// 	Infer({model: function(){
// 		map(function(obs_prm){
// 			return { p1: obs_prm.p1, p2: obs_prm.p2, quantifier: uniformDraw(quantifiers) }
// 		}, observed_premises)
// 	}})
// }

var MakeUniformDraw = function(lst) {
  return Categorical({
    vs: lst,
    ps: repeat(lst.length, function() {
      1
    })
  })
}

var alternative_quantifier_set_fn = cache(function(obs_prm) {
  MakeUniformDraw(map(function(q) {
    return {
      p1: obs_prm.p1,
      p2: obs_prm.p2,
      quantifier: q
    }
  }, quantifiers))
})

var alternative_quantifier_order_set_fn = cache(function(obs_prm) {
  MakeUniformDraw(map(function(q) {
    return {
      p1: obs_prm.p1,
      p2: obs_prm.p2,
      quantifier: q
    }
  }, quantifiers).concat({
    p1: obs_prm.p2,
    p2: obs_prm.p1,
    quantifier: obs_prm.quantifier
  }))
})

var alternative_set_maximal_fn = cache(function(obs_prm) {
  MakeUniformDraw(_.flatten(map(function(q) {
    return [{
        p1: obs_prm.p1,
        p2: obs_prm.p2,
        quantifier: q
      },
      {
        p1: obs_prm.p2,
        p2: obs_prm.p1,
        quantifier: q
      }
    ]
  }, quantifiers)))
})

var alternative_quantifier_set = cache(function(observed_premises) {
  map(alternative_quantifier_set_fn, observed_premises)
})

var alternative_quantifier_order_set = cache(function(observed_premises) {
  map(alternative_quantifier_order_set_fn, observed_premises)
})

var alternative_set_maximal = cache(function(observed_premises) {
  map(alternative_set_maximal_fn, observed_premises)
})



// var some_meaning = function(r){ return  r.region[x] ? r.region[y] : true }
// var vennToStringArray = function(venn){
//   return filter(map(function(v){
//     v.truth_val ? v.label : null
//   }, venn)
// }

var vennToString = function(venn){
  return filter(function(x){ x != null }, sort(map(function(v){
    v.truth_val ? v.label : null
  }, venn))).join(';')
}

// var acQUDToStringArray = function(venn){
//   var qudVal = map(function(v){
//       var label  = v.label
//       return v.truth_val ? label.replace("B", "") : null
//     }, venn)
//   // display(JSON.stringify(qudVal))
//   var filteredQudVal = filter(function(x){ x != null }, qudVal)
//   // display(JSON.stringify(filteredQudVal))
//   var uniqQudVal = _.uniq(filteredQudVal)
//   // display(JSON.stringify(uniqQudVal))
//   var sorted_uniqQudVal = sort(uniqQudVal)
//   // display(JSON.stringify(sorted_uniqQudVal))
//   return sorted_uniqQudVal.join(';')
// }

var qudToString = function(venn, qud){
  // display(venn)
  // display(venn)
  var dimensionToOmit = "ABC".replace(qud.slice(0,1), "").replace(qud.slice(1,2), "")
  var qudVal = map(function(v){
      var label  = v.label
      return v.truth_val ? label.replace(dimensionToOmit, "") : null
    }, venn)
  // display(JSON.stringify(qudVal))
  var filteredQudVal = filter(function(x){ x != null }, qudVal)
  // display(JSON.stringify(filteredQudVal))
  var uniqQudVal = _.uniq(filteredQudVal)
  // display(JSON.stringify(uniqQudVal))
  var sorted_uniqQudVal = sort(uniqQudVal)
  // display(JSON.stringify(sorted_uniqQudVal))
  return sorted_uniqQudVal.join(';')
}


var omitRegions = function(state, region_vals){
  return filter(function(s){ region_vals.indexOf(s.label) > -1 }, state)
}

var qudOmitRegions = function(state, region_vals){
  return vennToString(omitRegions(state, region_vals))
}

var qudFn = function(state, qud){
  return qud == "state" ? vennToString(state) :
        _.isArray(qud) ? qudOmitRegions(state, qud) :
                        qudToString(state, qud)

}

var conclusionQUDfn = function(venn){
  return map(function(utterance){
    var conclusionMeaningFn = lexicon[utterance.quantifier]
    return conclusionMeaningFn(venn, utterance.p1, utterance.p2)
  }, conclusions)
}


var possiblePremiseOnes = [
  {"p1":"A","p2":"B", "quantifier":"all"},
  {"p1":"A","p2":"B", "quantifier":"some"},
  {"p1":"A","p2":"B", "quantifier":"some_not"},
  {"p1":"A","p2":"B", "quantifier":"none"},
  {"p1":"B","p2":"A", "quantifier":"all"},
  {"p1":"B","p2":"A", "quantifier":"some"},
  {"p1":"B","p2":"A", "quantifier":"some_not"},
  {"p1":"B","p2":"A", "quantifier":"none"}
]


var sequentialQUDS = {
  'All;A;B': [ 'AB', 'ABC' ],
  'Some;A;B': [ 'A', 'AB', 'ABC', 'AC' ],
  'Some not;A;B': [ 'A', 'AB', 'ABC', 'AC' ],
  'None;A;B': [ 'A', 'AC', 'B', 'BC', 'C' ],
  'All;B;A': [ 'AB', 'ABC' ],
  'Some;B;A': [ 'AB', 'ABC', 'B', 'BC' ],
  'Some not;B;A': [ 'AB', 'ABC', 'B', 'BC' ],
  'None;B;A': [ 'A', 'AC', 'B', 'BC', 'C' ]
}


var isChar = function(str, char){
  return str == char
}

var firstTermPreference = function(syllogism){
  return (isChar(syllogism[0].p1, "A") && !(isChar(syllogism[1].p1, "C"))) ? "A" :
  (!isChar(syllogism[0].p1, "A") && (isChar(syllogism[1].p1, "C"))) ? "C" :
  false
}

var marginalizeVennForR = function(dist){
  Infer({model: function(){
    var venn = sample(dist);
    _.fromPairs(map(function(x){ return [x.label, x.truth_val] }, venn))
  }})
}

var regionExpectationFromVennDist = function(venn_dist){
  var seq_region_dist = Infer({model: function(){
    var venn = sample(venn_dist);
    _.fromPairs(map(function(x){ return [x.label, x.truth_val] }, venn))
  }})
  var vennRegionLabels = _.keys(seq_region_dist.support()[0])

  return map(function(s){
    return [s, expectation(Enumerate(function(){
      var venn = sample(seq_region_dist)
      return venn[s]
    })) ]
  }, vennRegionLabels)
}

// var

var marginalizeQUD_omitRegions = cache(function(dist, region_vals){
  Enumerate(function(){
    var state = sample(dist)
    omitRegions(state, region_vals)
  })
})

var marginalizeQUDval = cache(function(stateDist, qud){
  // display('marginalizedQudvl fn: qud = ' + qud)
  Infer({model: function(){
    var state = sample(stateDist)
    qudFn(state, qud)
    // qud == "state" ? vennToString(state): qudToString(stateDist, qud)
    // acQUDToStringArray(state)
  }})
})

// var qudFn = function(state, qud){
//   return qud == "state" ? vennToString(state) :
//         _.isArray(qud) ? qudOmitRegions(state, qud) :
//                         qudToString(state, qud)
//
// }

var regionExpectations_prior = regionExpectationFromVennDist(venn_space)


var alpha = 2
var noise = 0.01

var venn_interpreter = cache(function(sentences, pragmatics) {
  // display(JSON.stringify(sentences))
  Infer({
    model: function() {
      var venn = sample(venn_space)

      foreach(sentences, function(utterance) {
        var meaningFn = lexicon[utterance.quantifier]
        condition(flip(noise) ? true : meaningFn(venn, utterance.p1, utterance.p2))
      })
      // display('literal interpreter pre qud')
      var qudVal = qudFn(venn, pragmatics.qud)
      // display('literal interpreter post qud')

      return qudVal//pragmatics.production ? qudVal : venn

    },
    method: "enumerate"
  })
})

var literal_speaker = cache(function(venn){
  Infer({model: function(){
    var trueConclusions = filter(function(conclusion){
      var conclusionMeaningFn = lexicon[conclusion.quantifier]
      conclusionMeaningFn(venn, conclusion.p1, conclusion.p2)
    }, conclusions)

    var conclusion = uniformDraw(trueConclusions)
    return conclusion
  }})
})

var conclusion_prior =  Infer({
    model: function() {
      var venn = sample(venn_space)

      // foreach(premises, function(utterance) {
      //   var meaningFn = lexicon[utterance.quantifier]
      //   condition(meaningFn(venn, utterance.p1, utterance.p2))
      // })
      // var conclusion = sample(literal_speaker(venn))
      var conclusion = uniformDraw(conclusions)
      var conclusionMeaningFn = lexicon[conclusion.quantifier]
      condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))

      return formatSentence(conclusion)
    },
    method: "enumerate"
  })

var premise_speaker = cache(function(qudVal, observed_premises, pragmatics) {
  Infer({
    model: function() {
      var premises = map(sample, alternative_quantifier_set(observed_premises))
      // var premises = map(sample, alternative_quantifier_order_set(observed_premises))
      // var premises = map(sample, alternative_set_maximal(observed_premises))
      // display(JSON.stringify(venn))
      // var LiteralDist = qud.state ? venn_interpreter(premises, qud) :
      //   literal_reasoner(premises)
      var LiteralDist = venn_interpreter(premises, pragmatics)

      factor(alpha * LiteralDist.score(qudVal))

      return premises

    },
    method: "enumerate"
  })
})



var pragmatic_interpreter = cache(function(premises, pragmatics) {
  Infer({
    model: function() {
      var venn = sample(venn_space)
      var qudVal = qudFn(venn, pragmatics.qud)
      var SpeakerDist = premise_speaker(qudVal, premises, pragmatics)
      observe(SpeakerDist, premises)
      return pragmatics.production ? vennToString(venn) : venn
    },
    method: "enumerate"
  })
})

var single_pragmatic_interpreter = cache(function(premises) {

  var posterior_one = Infer({model: function() {
      var qud = "AB"
      var venn = sample(venn_space)
      var qudVal = qudFn(venn, qud)
      var SpeakerDist = single_premise_speaker(qudVal, premises[0], venn_space, qud)
      observe(SpeakerDist, premises[0])
      return venn
    }, method: "enumerate"})

    // var posterior_two = Infer({model: function() {
    //     var qud = "BC"
    //     var venn = sample(posterior_one)
    //     var qudVal = qud == "state" ? vennToString(venn) :
    //           qudToString(venn, qud)
    //     var SpeakerDist = single_premise_speaker(qudVal, premises[1], posterior_one, qud)
    //     observe(SpeakerDist, premises[1])
    //     return venn
    //   }, method: "enumerate"})

    return posterior_one

})


var venn_interpreter_custom_prior = cache(function(sentences, state_prior, qud) {
  // display(JSON.stringify(sentences))

  Infer({
    model: function() {
      var venn = sample(state_prior)
      // display(venn)
      foreach(sentences, function(utterance) {
        var meaningFn = lexicon[utterance.quantifier]
        condition(flip(noise) ? true : meaningFn(venn, utterance.p1, utterance.p2))
        // condition(meaningFn(venn, utterance.p1, utterance.p2))
      })
      // display(" in listener .. "+qud)
      var qudVal = qudFn(venn, qud)
      return qudVal
    },
    method: "enumerate"
  })
})

var single_premise_speaker = cache(function(qudVal, obs_premise, listener_prior, qud) {
  Infer({
    model: function() {
      var premise = sample(alternative_quantifier_set_fn(obs_premise))
      // display(JSON.stringify(premise))
      // display(venn)
      // display(qud)
      var LiteralDist = venn_interpreter_custom_prior([premise], listener_prior, qud)
      // display(JSON.stringify(LiteralDist))
      factor(alpha * LiteralDist.score(qudVal))
      return premise
    },
    method: "enumerate"
  })
})

var sequential_pragmatic_interpreter_qudOmitRegions = cache(function(premises, pragmatics) {

  var posterior_one = Infer({model: function() {
      var qud = "AB"
      var venn = sample(venn_space)
      var qudVal = qudFn(venn, qud)
      var SpeakerDist = single_premise_speaker(qudVal, premises[0], venn_space, qud)
      observe(SpeakerDist, premises[0])
      return venn
    }, method: "enumerate"})

    var regionExpectations_posterior = regionExpectationFromVennDist(posterior_one)
    var keys = map(first, regionExpectations_posterior)
    var qud = filter(function(k){
      _.fromPairs(regionExpectations_posterior)[k] >
      _.fromPairs(regionExpectations_prior)[k]
    }, keys)

  var qudDist = marginalizeQUD_omitRegions(posterior_one, qud)

  var posterior_two = Infer({model: function() {
      var qudVal = sample(qudDist) // ;
      // var venn = sample(posterior_one)
      // var qudVal = qudOmitRegions(venn, qud)
      display(qudVal)
      var prgmtics = {
        modular: pragmatics.modular,
        interpretation: pragmatics.interpretation,
        production: pragmatics.production,
        qud: qud
      }
      // var SpeakerDist = single_premise_speaker(qudVal, premises[1], qudDist, prgmtics.qud)
      var SpeakerDist = single_premise_speaker(qudFn(qudVal, "state"),
        premises[1], qudDist, "state")
      // display(JSON.stringify(SpeakerDist))
      // displayDist(SpeakerDist)
      observe(SpeakerDist, premises[1])
      // return qudVal
      // return  qudVal // pragmatics.production ? vennToString(qudVal) : qudVal
      return  pragmatics.production ? vennToString(qudVal) : qudVal
    }, method: "enumerate"})

    return {posterior_two, qud}

})

var qud_pragmatic_reasonser = cache(function(premises) {
  // display("pragmatic interpreter")
  // display(JSON.stringify(premises))
  Infer({
    model: function() {
      var venn = sample(venn_space)

      var trueConclusions = filter(function(conclusion){
        var conclusionMeaningFn = lexicon[conclusion.quantifier]
        conclusionMeaningFn(venn, conclusion.p1, conclusion.p2)
      }, conclusions)

      var conclusion = uniformDraw(trueConclusions)
      var qudVal = formatSentence(conclusion)

      // var qudVal = qud == "conclusion" ? acQUDToStringArray(venn) : venn
      // display(JSON.stringify(qudVal))
      // display(JSON.stringify(vennToStringArray(venn)))
      // +
      // JSON.stringify(_.map(venn, "truth_val")))
      var SpeakerDist = argument_speaker(qudVal, premises, "conclusion")
      // display('after speak')
      observe(SpeakerDist, premises)
      return formatSentence(conclusion)
    },
    method: "enumerate"
  })
})

var conclusion_speaker = function(qudVal, qud) {
  Infer({
    model: function() {
      var conclusion = uniformDraw(conclusions)
      var ConclusionInterpretation = venn_interpreter([conclusion], qud)
      factor(alpha * ConclusionInterpretation.score(qudVal))
      return conclusion
    },
    method: "enumerate"
  })
}

var pragmatic_interpreter_conclusion = cache(function(conclusion, qud){
  Infer({model: function(){
    var venn = sample(venn_space)
    var qudVal = qudFn(venn, qud)

    var SpeakerDist = conclusion_speaker(qudVal, qud)
    observe(SpeakerDist, conclusion)
    return qudVal
  }, method: "enumerate"})
})

var conclusion_speaker = function(premises) {
  Infer({
    model: function() {
      var PremiseInterpretation = venn_interpreter(premises)
      var venn = sample(PremiseInterpretation)

      var conclusion = uniformDraw(conclusions)
      var ConclusionInterpretation = venn_interpreter([conclusion])

      factor(alpha * ConclusionInterpretation.score(venn))

      return conclusion
    }
  })
}

var literal_reasoner = cache(function(sentences) {
  Infer({
    model: function() {
      var venn = sample(venn_space)

      foreach(sentences, function(utterance) {
        var meaningFn = lexicon[utterance.quantifier]
        condition(flip(0.03) ? true : meaningFn(venn, utterance.p1, utterance.p2))
      })

      var conclusion = sample(literal_speaker(venn))
      return formatSentence(conclusion)
    },
    method: "enumerate"
  })
})

var arguer = cache(function(qudVal, observed_premises) {
  Infer({
    model: function() {
      var premises = map(sample, alternative_quantifier_set(observed_premises))
      // var premises = map(sample, alternative_quantifier_order_set(observed_premises))
      // var premises = map(sample, alternative_set_maximal(observed_premises))
      var LiteralDist = literal_reasoner(premises)
      factor(alpha * LiteralDist.score(qudVal))
      return premises
    },
    method: "enumerate"
  })
})

var integrated_pragmatic_reasoner = function(premises, interpretationOnly) {
  Infer({
    model: function() {

      var venn = sample(venn_space)
      var conclusion = uniformDraw(conclusions)
      // var conclusion = sample(literal_speaker(venn))
      var conclusionMeaningFn = lexicon[conclusion.quantifier]
      condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))
      var ArgumentDist = arguer(formatSentence(conclusion), premises)
      observe(ArgumentDist, premises)

      return interpretationOnly ? venn : formatSentence(conclusion)

    }
  })
}


var integrated_pragmatic_reasoner_wProduction = function(premises, pragmatics) {
  Infer({
    model: function() {

      var PremiseInterpretation = integrated_pragmatic_reasoner(premises, true)
      var qudValDist = marginalizeQUDval(PremiseInterpretation)
      // display(JSON.stringify(qudValDist))
      var conclusion = uniformDraw(conclusions)

      var ConclusionInterpretation = pragmatics.production.depth == 0 ?
                venn_interpreter([conclusion], "AC") :
                pragmatic_interpreter_conclusion(conclusion, "AC")

      // display(JSON.stringify(sort(ConclusionInterpretation.support())))

      var _kl = KL(qudValDist, ConclusionInterpretation, qudValDist.support())
      // display(formatSentence(conclusion) + " _ " + _kl)
      factor(alpha * -1 * _kl)

      return formatSentence(conclusion)

    }
  })
}

var rr_dist = cache(function(dist, n_samples){
  Infer({model: function(){
    var venns = repeat(n_samples, function(){ sample(dist) })
    var uniq_venns = sort(_.uniq(venns)) // at this point, could map onto the equivalence class of true quantifier sentences
    condition(uniq_venns.length == n_samples)
    return uniq_venns
  }, method: "enumerate"})
})

var rr_pragmatic_reasoner = function(premises, pragmatics, n_samples) {
  Infer({
    model: function() {

      var PremiseInterpretation = pragmatics.interpretation ?
        pragmatic_interpreter(premises) :
        venn_interpreter(premises)

      var samples = repeat(n_samples, function(){ sample(PremiseInterpretation) })
      // var rrPremiseInterpretation = rr_dist(PremiseInterpretation, n_samples)
      // var samples = sample(rrPremiseInterpretation)
      // var samples = _.sampleSize(PremiseInterpretation.support(), n_samples)
      // display(JSON.stringify(samples))

      _.fromPairs(map(function(conclusion){
        var ConclusionInterpretation = venn_interpreter([conclusion])
        // display(JSON.stringify(ConclusionInterpretation.support()))
        var scrs = map(function(venn_diagram){
          var scr = ConclusionInterpretation.score(venn_diagram)
          // display(venn_diagram + " " + scr)
          return scr // == -Infinity ? Math.log(Number.EPSILON) : scr //+ Number.EPSILON
        }, samples)

        return [formatSentence(conclusion), alpha * sum(scrs)]

      }, conclusions))

      // var conclusion = uniformDraw(conclusions)
      // // display(conclusion)
      // if (pragmatics.production) {
      //
      //   var ConclusionInterpretation = venn_interpreter([conclusion])
      //   // display(JSON.stringify(ConclusionInterpretation.support()))
      //   var scrs = map(function(venn_diagram){
      //     var scr = ConclusionInterpretation.score(venn_diagram)
      //     // display(venn_diagram + " " + scr)
      //     return scr
      //   }, samples)
      //
      //   factor(alpha * sum(scrs))
      //
      //   // var rrConclusionInterpretation = rr_dist(ConclusionInterpretation, n_samples)
      //
      //   // var _kl = KL(PremiseInterpretation, ConclusionInterpretation, PremiseInterpretation.support())
      //   // factor(alpha * -1 * _kl)
      //   // factor(alpha * rrConclusionInterpretation.score(state))
      //
      // } else {
      //   var venn = sample(PremiseInterpretation)
      //
      //   var conclusionMeaningFn = lexicon[conclusion.quantifier]
      //   condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))
      //
      // }
      // //
      // return formatSentence(conclusion)
      // return state
    },
    method: "forward", samples: 1000
  })
}

var marginalizeConclusions = cache(function(rr_dist){
  _.fromPairs(map(function(conclusion){
    return [formatSentence(conclusion), expectation(Infer({model: function(){
      var s = sample(rr_dist)
      var c = formatSentence(uniformDraw(conclusions))
      // display(JSON.stringify(s[conclusion]))
      // display(c + ", " + s[c] + ", " + (s[c] == undefined ) )
      // factor(s[conclusion])
      // factor(s[formatSentence(conclusion)] ? s[formatSentence(conclusion)] : -Infinity)
      var scr = s[formatSentence(conclusion)]
      return scr //scr == -Infinity ? Math.log(0.01) : scr
    }, method: "enumerate"}))]
  }, conclusions))
})

var modular_pragmatic_reasoner = function(premises, pragmatics) {
  Infer({
    model: function() {

      var PremiseInterpretation = pragmatics.interpretation ?
            pragmatics.qud == "seq" ?
              sequential_pragmatic_interpreter_qudOmitRegions(premises, pragmatics) :
              pragmatic_interpreter(premises, pragmatics) :
              venn_interpreter(premises, pragmatics)

      var qudDist_prior = marginalizeQUD_omitRegions(venn_space, PremiseInterpretation.qud)
      display('after interpretation')
      if (pragmatics.production) {
        display('enter pragmatic production')
        var conclusion = uniformDraw(conclusions)
        // var qudValDist = marginalizeQUDval(PremiseInterpretation, "AC")
        // display(JSON.stringify())
        display('enter listener in production')
        // var ConclusionInterpretation = venn_interpreter([conclusion], {production: true, qud: "state"})
        var ConclusionInterpretation = venn_interpreter_custom_prior([conclusion],
          qudDist_prior, "state")

// pragmatics.production.depth == 0 ?
//                   venn_interpreter([conclusion], {production: true, qud: "state"}) :
//                   pragmatic_interpreter_conclusion(conclusion, "AC")

        display('exit listener in production')
        var _kl = KL(PremiseInterpretation.posterior_two, ConclusionInterpretation, PremiseInterpretation.posterior_two.support())

        // var _kl = KL(PremiseInterpretation, ConclusionInterpretation, PremiseInterpretation.support())
        factor(alpha * -1 * _kl)
        // factor(alpha * ConclusionInterpretation.score(vennToStringArray(venn)))
        return formatSentence(conclusion)

      } else {
        // display('lit production')
        var venn = sample(pragmatics.qud == "seq" ? PremiseInterpretation.posterior_two : PremiseInterpretation)
        var conclusion = sample(literal_speaker(venn))
        // display(JSON.stringify(venn))
        // var conclusion = uniformDraw(conclusions)
        // var conclusionMeaningFn = lexicon[conclusion.quantifier]
        // condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))
        return formatSentence(conclusion)

      }


    }
  })
}


var syllogism = [
  {"p1":"A","p2":"B", "quantifier":"all"},
  {"p1":"B","p2":"C", "quantifier":"some"}
]


// var returnVal = {
//   literal: marginalizeVennForR(venn_interpreter(syllogism, "state")),
//   pragmatic: marginalizeVennForR(pragmatic_interpreter(syllogism, "state")),
//   sequential: marginalizeVennForR(sequential_pragmatic_interpreter_qudOmitRegions(syllogism))
// }
// returnVal;
var pragmatics = {
  modular: true,
  interpretation: true,
  production: true,// {
    // depth: 0
  // },
  qud: "seq"
}

displayDist(
  modular_pragmatic_reasoner(syllogism, pragmatics)
)
// var x = sequential_pragmatic_interpreter_qudOmitRegions(syllogism, pragmatics)
// displayDist(x)
// displayDist(x.posterior_two)
// x.qud
// FOR SINGLE PREMISE INTERPRETATIONS
// _.fromPairs(map(function(u){
//
//   return [ formatSentence(u),  {
//     literal: marginalizeVennForR( venn_interpreter([u], "state")  ),
//     pragmatic: marginalizeVennForR( single_pragmatic_interpreter([u], "state")  )
//   } ]
//
// }, possiblePremiseOnes))


// FOR COMPUTING REGIONS WITH INCREASED PROBABILITY FOLLOWING PREMISE_1
// FOR CACHEING QUD in SEQ QUD MODEL
// _.fromPairs(map(function(u){
//
//   var regionExpectations_posterior = sort(regionExpectationFromVennDist(
//     venn_interpreter([u], "state")
//   ))
//   var keys = map(first, regionExpectations_posterior)
//   var qud = filter(function(k){
//     _.fromPairs(regionExpectations_posterior)[k] >
//     _.fromPairs(regionExpectations_prior)[k]
//   }, keys)
//
//   return [formatSentence(u), qud]
//
// }, possiblePremiseOnes))





// [ 'C', 'B', 'BC', 'A', 'AC' ]

// venn_interpreter(syllogism, "state").support()
// display()
// var postOne = sequential_pragmatic_interpreter_qudOmitRegions(syllogism)
// var regionExpectations_prior = regionExpectationFromVennDist(venn_space)
// var regionExpectations_posterior = regionExpectationFromVennDist(postOne)
// display(regionExpectations_prior)
// display(regionExpectations_posterior)
// var keys = map(first, regionExpectations_posterior)
// display(keys)


// displayDist(integrated_pragmatic_reasoner(syllogism))

// display(syllogism)

// var literalVennDist = venn_interpreter(syllogism, "state")
// var vennDist = pragmatic_interpreter(syllogism, "state")
// var sequentialIntDist = sequential_pragmatic_interpreter_qudOmitRegions(syllogism)
// sequentialIntDist

// sequentialIntDist

// displayDist(modular_pragmatic_reasoner(syllogism,
//   // { interpretation: true, production: false, qud: "state"}
//   { interpretation: true, production: {depth: 0}, qud: "AC"}
// ))
//
// // displayDist(marginalizeVennForR(sequentialIntDist))
// displayDist(modular_pragmatic_reasoner(syllogism,
//   // { interpretation: true, production: false, qud: "state"}
//   { interpretation: "sequential_qud", production: {depth: 0}, qud: "AC"}
// ))


// sequential_pragmatic_interpreter_qudOmitRegions(
//   syllogism
// )
// displayDist(marginalizeVennForR(marginalizeQUD_omitRegions(sequentialIntDist, topNregions)))
// // //






// sample(vennDist);

// var conclusionObj = marginalizeConclusions(
//   // displayDist(
//     rr_pragmatic_reasoner(syllogism, { interpretation: false, production: true }, 3)
//   // )
// )
// //
// // conclusionObj
//
// display(map(first, _.toPairs(conclusionObj)))
// normalize(map(function(x){ return Math.exp(second(x)) } , _.toPairs(conclusionObj)))





// displayDist(
//   integrated_pragmatic_reasoner_wProduction(syllogism,
//   { production: {depth: 0} }
//   )
// )
// displayDist(literal_reasoner(syllogism))


// displayDist(conclusion_prior)
// displayDist(qud_pragmatic_reasonser(syllogism))
// displayDist(
//   marginalizeConclusions(
//     rr_pragmatic_reasoner(syllogism, { interpretation: false, production: true }, 10)
//   )
// )
// venn_interpreter([{ p1: 'A', p2: 'C', quantifier: 'nvc' }]).score('ABC;BC;B')



// display("== conclusion speaker rv2 ==")
// repeat(10, function(){
//   displayDist(rr_pragmatic_reasoner(syllogism, { interpretation: false, production: true }, 10))
//
// })

// ven
// var x = venn_interpreter([{ p1: 'A', p2: 'C', quantifier: 'some' }])
// JSON.stringify(x.support())
// rr_dist(x, 3).support()
// repeat(2, function(){ sample(x) })
// rr_pragmatic_reasoner(syllogism, { interpretation: false, production: true }, 3).support()
// displayDist(conclusion_prior)

// venn_interpreter(syllogism).support()

// displayDist(venn_interpreter(syllogism))
// display('conclucusion interpretation')
// displayDist(venn_interpreter([{"p1":"A","p2":"C", "quantifier":"nvc"}]))
// display("== pragmatic interpretation ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: true, production: false }))



// var venn = [{
//   "region": {
//     "A": false,
//     "B": false,
//     "C": false
//   },
//   "priorProb": 0.5,
//   "truth_val": false,
//   "label": ""
// }, {
//   "region": {
//     "A": false,
//     "B": false,
//     "C": true
//   },
//   "priorProb": 0.5,
//   "truth_val": true,
//   "label": "C"
// }, {
//   "region": {
//     "A": false,
//     "B": true,
//     "C": false
//   },
//   "priorProb": 0.5,
//   "truth_val": true,
//   "label": "B"
// }, {
//   "region": {
//     "A": false,
//     "B": true,
//     "C": true
//   },
//   "priorProb": 0.25,
//   "truth_val": true,
//   "label": "BC"
// }, {
//   "region": {
//     "A": true,
//     "B": false,
//     "C": false
//   },
//   "priorProb": 0.5,
//   "truth_val": true,
//   "label": "A"
// }, {
//   "region": {
//     "A": true,
//     "B": false,
//     "C": true
//   },
//   "priorProb": 0.25,
//   "truth_val": true,
//   "label": "AC"
// }, {
//   "region": {
//     "A": true,
//     "B": true,
//     "C": false
//   },
//   "priorProb": 0.25,
//   "truth_val": true,
//   "label": "AB"
// }, {
//   "region": {
//     "A": true,
//     "B": true,
//     "C": true
//   },
//   "priorProb": 0.125,
//   "truth_val": true,
//   "label": "ABC"
// }]

// displayDist(argument_speaker(venn, syllogism))
// displayDist(venn_interpreter(syllogism))
// display("== literal reasoner v2 ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: false, production: false }))
//

//
// display("== conclusion speaker rv2 ==")
// displayDist(literal_reasoner(syllogism))


// display("== full pragmatic reasoner ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: true, production: true }))

// formatQuantifier("Some not")
// levels(df, "premise_1")
// map(stringToSyllogism, levels(df, "premise_2"))
// _.filter(df, {premise_1: undefined})

// var venn = [
//   { region: { A: false, B: false, C: false }, truth_val: false },
//   { region: { A: false, B: false, C: true }, truth_val: false },
//   { region: { A: false, B: true, C: false }, truth_val: false },
//   { region: { A: false, B: true, C: true }, truth_val: false },
//   { region: { A: true, B: false, C: false }, truth_val: false },
//   { region: { A: true, B: false, C: true }, truth_val: false },
//   { region: { A: true, B: true, C: false }, truth_val: true },
//   { region: { A: true, B: true, C: true }, truth_val: false }
// ]
//
// displayDist(
// 	argument_speaker(venn)
// )


// var utterance = {p1: "A", p2: "B", quantifier: "all"}
// var myfn = lexicon[utterance.quantifier]
// myfn()
// repeat(2, function(){ uniformDraw(quantifiers )})



// map(function(region){
// 	map(function(r){r[1] ? r[0] : "" }, _.toPairs(region)).join("")
// }, regions)

// listener0.support().length

// var venn = sample(venn_space)

// // condition(any(some_filter("A", "B"), venn))
// display(all(all_filter("A", "B"), venn))

// map(all_filter("A", "B"), venn)
// var f = all_filter("A", "B")
// f(venn[7])
// venn[7]
// venn
// var relevant_regions = filter(some_filter("A", "B"), venn)
// venn_space.support().length
// display(JSON.stringify())
// display(any(function(r){ r.truth_val }, relevant_regions))
// relevant_regions
// regions
// var shades_of_regions = Enumerate(function(){
// 	var region = sample(regions)
// 	var prob = uniformDraw([0, 0.5, 1])
// 	return {region, prob}
// })


// var quantifier_meaning = {
// 	"Some": function(x, y){ return }
// }
// shades_of_regions.support()

// var some_meaning = function(x, y){ return function(r){ return r[x] && r[y] }}
//
// // var r = sample(regions)
// var r = { A: true, B: true, C: true }
// display(r)
// some_meaning("A", "B")(r)
