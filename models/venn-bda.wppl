// webppl venn.wppl --require webppl-csv

var args = process.argv
var chain = last(args) // load index as last command line index

var pragmatics = {
  interpretation: args[args.length - 3] == "literal" ? false : true,
  production: args[args.length - 2] == "literal" ? false : true,
}

var foreach = function(lst, fn) {
  var foreach_ = function(i) {
    if (i < lst.length) {
      fn(lst[i]);
      foreach_(i + 1);
    }
  };
  foreach_(0);
};

var dataFrame = function(rawCSV) {
  return map(function(row) {
    return _.fromPairs(_.zip(rawCSV[0], row))
  }, rawCSV.slice(1))
}

var levels = function(df, label) {
  return _.uniq(_.map(df, label));
}

var displayDist = function(dist) {
  foreach(dist.support(), function(x) {
    display(x + ", " + Math.exp(dist.score(x)))
  })
}

var regions = Enumerate(function() {
  return {
    A: flip(),
    B: flip(),
    C: flip()
  }
}).support()


var distProbs = function(dist, supp) {
  return map(function(s) {
    return Math.exp(dist.score(s))
  }, supp)
}

var KL = function(p, q, supp) {
  var P = distProbs(p, supp),
    Q = distProbs(q, supp);
  var diverge = function(xp, xq) {
    return xp == 0 ? 0 : (xp * Math.log(xp / xq));
  };
  return sum(map2(diverge, P, Q));
};


// var venn_space = Enumerate(function(){
// 	map(function(region){ return {region: region, truth_val: flip() } }, regions)
// })


var parseVennDiagram = function(r) {
  map(function(i) {
    i[1] ? i[0] : ""
  }, _.toPairs(r)).join("")
}

var regionLabels = map(parseVennDiagram, regions)

var iidProbs = {
  // "": 0.5,
  "": 0.5,
  "A": 0.5,
  "B": 0.5,
  "C": 0.5,
  "AB": 0.25,
  "BC": 0.25,
  "AC": 0.25,
  "ABC": 0.125
}

var correlatedProbs = {
  "": 0.5,
  "A": 0.25,
  "B": 0.25,
  "C": 0.25,
  "AB": 0.50,
  "BC": 0.50,
  "AC": 0.50,
  "ABC": 0.75
}

var flatProbs = {
  "": 0.5,
  "A": 0.5,
  "B": 0.5,
  "C": 0.5,
  "AB": 0.5,
  "BC": 0.5,
  "AC": 0.5,
  "ABC": 0.5
}

var regionProbs = iidProbs;

var venn_space = Enumerate(function() {
  map(function(region) {
    var regionLabel = parseVennDiagram(region)
    var priorProb = regionProbs[regionLabel]
    return {
      region: region,
      priorProb: priorProb,
      truth_val: flip(priorProb),
      label: parseVennDiagram(region)
    }
  }, regions)
})

var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}


// SHOULD BE PAIRED WITH all()
// this DOES NOT includes an existential presupposition
var all_filter = function(x, y) {
  return function(r) {
    return r.region[x] ? // if its an A
      !r.region[y] ? // and not B
      !r.truth_val : // then it must be false
      true : // if it's an A and not a B, then it must be false
      true // if it's not an A, then it doesn't affect overall truthval (because we will use "all" to search that all are positive truth val)
  }
}

// SHOULD BE PAIRED WITH any()
// this includes an existential presupposition
var some_filter = function(x, y) {
  return function(r) {
    return r.region[x] ? // if its an A
      r.region[y] ? // and a B
      r.truth_val : // then this is the relevant region, and select its truthval
      false : // if it's an A and not a B, then it doesn't contribute a positive truth val (because we will use "any" to search for any positive truth val)
      false // if it's not an A, then it doesn't contribute a positive truth val (because we will use "any" to search for any positive truth val)
  }
}

var lexicon = {
  all: function(state, property1, property2) {
    return (all(all_filter(property1, property2), state) &&
      any(some_filter(property1, property2), state))
  },
  some: function(state, property1, property2) {
    return any(some_filter(property1, property2), state)
  },
  none: function(state, property1, property2) {
    return !(any(some_filter(property1, property2), state))
  },
  some_not: function(state, property1, property2) {
    return !(all(all_filter(property1, property2), state) &&
      any(some_filter(property1, property2), state))
  },
  nvc: function(state, property1, property2) {
    return true
  }
}

var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}
var predicate_filter = function(x, y) {
  return function(r) {
    return _.fromPairs([
      [x, r.region[x]],
      [y, r.region[y]]
    ])
  }
}
// var listener0 = Enumerate(function(){
// 	var state = sample(venn_space)
// 	var predicate_filter = function(x, y){
// 		filter(function(r){return { region: }}, state)
// 	}
// 	condition(state.)
// })

var quantifiers = ["all", "some", "none", "some_not"]

// var conclusions = ["all", "some", "none", "some_not", "nvc"]

var conclusions = [{
    p1: "A",
    p2: "C",
    quantifier: "all"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "some"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "none"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "some_not"
  },
  {
    p1: "C",
    p2: "A",
    quantifier: "all"
  },
  {
    p1: "C",
    p2: "A",
    quantifier: "some_not"
  },
  {
    p1: "A",
    p2: "C",
    quantifier: "nvc"
  }
]

// var PremisePrior = cache(function(observed_premises){
// 	Infer({model: function(){
// 		map(function(obs_prm){
// 			return { p1: obs_prm.p1, p2: obs_prm.p2, quantifier: uniformDraw(quantifiers) }
// 		}, observed_premises)
// 	}})
// }

var MakeUniformDraw = function(lst) {
  return Categorical({
    vs: lst,
    ps: repeat(lst.length, function() {
      1
    })
  })
}

var alternative_quantifier_set = cache(function(observed_premises) {
  map(function(obs_prm) {
    MakeUniformDraw(map(function(q) {
      return {
        p1: obs_prm.p1,
        p2: obs_prm.p2,
        quantifier: q
      }
    }, quantifiers))
  }, observed_premises)
})


var alternative_quantifier_order_set = cache(function(observed_premises) {
  map(function(obs_prm) {
    MakeUniformDraw(map(function(q) {
      return {
        p1: obs_prm.p1,
        p2: obs_prm.p2,
        quantifier: q
      }
    }, quantifiers).concat({
      p1: obs_prm.p2,
      p2: obs_prm.p1,
      quantifier: obs_prm.quantifier
    }))
  }, observed_premises)
})

var alternative_set_maximal = cache(function(observed_premises) {
  map(function(obs_prm) {
    MakeUniformDraw(_.flatten(map(function(q) {
      return [{
          p1: obs_prm.p1,
          p2: obs_prm.p2,
          quantifier: q
        },
        {
          p1: obs_prm.p2,
          p2: obs_prm.p1,
          quantifier: q
        }
      ]
    }, quantifiers)))
  }, observed_premises)
})



// var some_meaning = function(r){ return  r.region[x] ? r.region[y] : true }






var syllogism = [{
    p1: "A",
    p2: "B",
    quantifier: "all"
  },
  {
    p1: "B",
    p2: "C",
    quantifier: "all"
  }
]


// display("== literal reasoner v2 ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: false, production: false }))
//
// display("== conclusion speaker rv2 ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: false, production: true }))
//
// display("== pragmatic interpretation ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: true, production: false }))
//
// display("== full pragmatic reasoner ==")
// displayDist(full_pragmatic_reasoner(syllogism, { interpretation: true, production: true }))


var formatSentence = function(sentence) {

  var quantifier = (sentence.quantifier == "nvc") ? "NVC" :
  (sentence.quantifier == "some_not") ? "Some not" :
   sentence.quantifier.charAt(0).toUpperCase() + sentence.quantifier.slice(1)

  return quantifier == "NVC" ? "NVC;" : quantifier + ";" + sentence.p1 + ";" + sentence.p2
}

var formatQuantifier = function(q) {
  q.toLowerCase().split(' ').join("_")
}

var stringToSyllogism = function(str) {
  _.fromPairs(_.zip(["quantifier", "p1", "p2"],
    mapIndexed(function(x, y) {
      return x == 0 ? formatQuantifier(y) : y
    }, str.split(';'))))
}

var df = dataFrame(csv.read("ccobra_data/ragni2016_collapse.csv"))

var df_clean = map(function(row){
  extend(row, {
      prem_1: stringToSyllogism(row.premise_1),
      prem_2: stringToSyllogism(row.premise_2),
      c_clean: stringToSyllogism(row.conclusion)
    })
}, filter(function(x){
  x.premise_1 !== undefined
}, df))

var premise_1s = map(stringToSyllogism,
  filter(function(x) {
    x !== undefined
  }, levels(df, "premise_1"))
)
var premise_2s = map(stringToSyllogism,
  filter(function(x) {
    x !== undefined
  }, levels(df, "premise_2"))
)

var premise_1s = levels(df_clean, "prem_1")
var premise_2s = levels(df_clean, "prem_2")


// premise_1s

var bda_model = function(){

	var speakerOptimality = {
		alpha_1 : uniformDrift({a: 0, b: 20, width: 2}),
		alpha_2 : uniformDrift({a: 0, b: 20, width: 2})
	}

  var regionProbs = repeat(8, function(){ return uniformDrift({a: 0, b: 1, width: 0.1}) })

  var venn_space = Enumerate(function() {
    map2(function(region, priorProb) {
      var regionLabel = parseVennDiagram(region)
      return {
        region: region,
        priorProb: priorProb,
        truth_val: flip(priorProb),
        label: parseVennDiagram(region)
      }
    }, regions, regionProbs)
  })

  var venn_interpreter = cache(function(sentences) {
    Infer({
      model: function() {
        var venn = sample(venn_space)

        foreach(sentences, function(utterance) {
          var meaningFn = lexicon[utterance.quantifier]
          condition(meaningFn(venn, utterance.p1, utterance.p2))
        })

        return venn
      },
      method: "enumerate"
    })
  })

  var literal_reasoner = cache(function(premises) {
    Infer({
      model: function() {
        var venn = sample(venn_space)

        foreach(premises, function(utterance) {
          var meaningFn = lexicon[utterance.quantifier]
          condition(meaningFn(venn, utterance.p1, utterance.p2))
        })

        var conclusion = uniformDraw(conclusions)
        var conclusionMeaningFn = lexicon[conclusion.quantifier]
        condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))

        return conclusion
      },
      method: "enumerate"
    })
  })

  var argument_speaker = cache(function(venn, observed_premises) {
    Infer({
      model: function() {

        // var premises = [
        // 	{p1: "A", p2: "B", quantifier: uniformDraw(quantifiers ) },
        // 	{p1: "B", p2: "C", quantifier: uniformDraw(quantifiers ) }
        // ]
        // var premises = map(sample, alternative_quantifier_set(observed_premises))
        var premises = map(sample, alternative_quantifier_order_set(observed_premises))
        // var premises = map(sample, alternative_set_maximal(observed_premises))

        var LiteralDist = venn_interpreter(premises)

        factor(speakerOptimality.alpha_1 * LiteralDist.score(venn))

        return premises

      },
      method: "enumerate"
    })
  })

  var pragmatic_interpreter = cache(function(premises) {
    display("pragmatic interpreter")
    Infer({
      model: function() {
        var venn = sample(venn_space)
        var SpeakerDist = argument_speaker(venn, premises)
        observe(SpeakerDist, premises)
        return venn
      }
    })
  })


  var conclusion_speaker = function(premises) {
    Infer({
      model: function() {
        var PremiseInterpretation = venn_interpreter(premises)
        var venn = sample(PremiseInterpretation)

        var conclusion = uniformDraw(conclusions)
        var ConclusionInterpretation = venn_interpreter([conclusion])

        factor(speakerOptimality.alpha_2 * ConclusionInterpretation.score(venn))

        return conclusion
      }
    })
  }


  var full_pragmatic_reasoner = function(premises, pragmatics) {
    Infer({
      model: function() {

        var PremiseInterpretation = pragmatics.interpretation ?
          pragmatic_interpreter(premises) :
          venn_interpreter(premises)


        var conclusion = uniformDraw(conclusions)
        var venn = sample(PremiseInterpretation)

        if (pragmatics.production) {

          var ConclusionInterpretation = venn_interpreter([conclusion])

          // var _kl = KL(PremiseInterpretation, ConclusionInterpretation, ConclusionInterpretation.support())
          // factor(alpha * -1 * _kl)
          factor(speakerOptimality.alpha_2 * ConclusionInterpretation.score(venn))

        } else {

          var conclusionMeaningFn = lexicon[conclusion.quantifier]
          condition(conclusionMeaningFn(venn, conclusion.p1, conclusion.p2))

        }

        return formatSentence(conclusion)

      }
    })
  }




	foreach(premise_1s, function(prem_1){
			foreach(premise_2s, function(prem_2){

        // var data = _.filter(df, {premise_1: prem_1}),
        //     interpretation: _.filter(data.interpretation, {distribution: itemName})//,
        //     // observation: _.filter(data.observation, {property: item})
        // };

				var syllogisticPremises = [prem_1, prem_2]

        var rsaPredictions = full_pragmatic_reasoner(syllogisticPremises, pragmatics)

        foreach(rsaPredictions.support(), function(s){

          query.add([formatSentence(prem_1), formatSentence(prem_2), s], Math.exp(rsaPredictions.score(s)))

        })



			})
	})

}


df_clean
// regionLabels
// formatQuantifier("Some not")
// levels(df, "premise_1")
// map(stringToSyllogism, levels(df, "premise_2"))
// _.filter(df, {premise_1: undefined})

// var venn = [
//   { region: { A: false, B: false, C: false }, truth_val: false },
//   { region: { A: false, B: false, C: true }, truth_val: false },
//   { region: { A: false, B: true, C: false }, truth_val: false },
//   { region: { A: false, B: true, C: true }, truth_val: false },
//   { region: { A: true, B: false, C: false }, truth_val: false },
//   { region: { A: true, B: false, C: true }, truth_val: false },
//   { region: { A: true, B: true, C: false }, truth_val: true },
//   { region: { A: true, B: true, C: true }, truth_val: false }
// ]
//
// displayDist(
// 	argument_speaker(venn)
// )


// var utterance = {p1: "A", p2: "B", quantifier: "all"}
// var myfn = lexicon[utterance.quantifier]
// myfn()
// repeat(2, function(){ uniformDraw(quantifiers )})



// map(function(region){
// 	map(function(r){r[1] ? r[0] : "" }, _.toPairs(region)).join("")
// }, regions)

// listener0.support().length

// var venn = sample(venn_space)

// // condition(any(some_filter("A", "B"), venn))
// display(all(all_filter("A", "B"), venn))

// map(all_filter("A", "B"), venn)
// var f = all_filter("A", "B")
// f(venn[7])
// venn[7]
// venn
// var relevant_regions = filter(some_filter("A", "B"), venn)
// venn_space.support().length
// display(JSON.stringify())
// display(any(function(r){ r.truth_val }, relevant_regions))
// relevant_regions
// regions
// var shades_of_regions = Enumerate(function(){
// 	var region = sample(regions)
// 	var prob = uniformDraw([0, 0.5, 1])
// 	return {region, prob}
// })


// var quantifier_meaning = {
// 	"Some": function(x, y){ return }
// }
// shades_of_regions.support()

// var some_meaning = function(x, y){ return function(r){ return r[x] && r[y] }}
//
// // var r = sample(regions)
// var r = { A: true, B: true, C: true }
// display(r)
// some_meaning("A", "B")(r)
