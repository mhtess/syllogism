---
title: "syll-tfbt"
author: "mht"
date: "October 31, 2014"
output: html_document
---

# The Full Bayesian Thing for syllogisms


```{r dataload, echo=FALSE}
library(reshape2)
library(ggplot2)
library(plyr)

library(R2jags)
library(gridExtra)

map_radio_to_continuous <- function(rad,cts){50+(2*rad-1)*(cts/2)}

# Load data from 2 experiments
fpath = '/Users/mht/Documents/research/syllogism/data/03syllogism_reasoning/'
fpath2 = '/Users/mht/Documents/research/syllogism/data/04syllogism_reasoning/'

df<-read.csv((paste(fpath,'syllbelief-exp-mturk_all_n250.csv',sep="")))
df$experiment <- factor(1)

df2<-read.csv((paste(fpath2,'syllbelief-exp2-mturk.csv',sep="")))
df2$condition<-paste(df2$condition,'2',sep='.')
df2$experiment <- factor(2)

exp1sylls<-levels(df$syll)
exp2sylls<-levels(df2$syll)
resp_labels<-c("Q_A","Q_E","Q_I","Q_O")


df<-subset(df,rt<mean(df$rt)+2*sqrt(var(df$rt)))
df2<-subset(df2,rt<mean(df2$rt)+2*sqrt(var(df2$rt)))

df2$subj<- factor(df2$subj, labels=as.integer(substring(levels(df2$subj),2,4))+length(levels(df$subj)))

df.c <- rbind(df,df2)
df.c$condition<-factor(df.c$condition,levels=c('slide','radio','radio.2'))

# map radio + slider to just radio
for (i in 1:length(df.c$subj)){
  if (substring(df.c[i,]$condition,1,5)=='radio'){
    df.c[i,]$Q_A <- map_radio_to_continuous(df.c[i,]$radio_A,df.c[i,]$Q_A)
    df.c[i,]$Q_E <- map_radio_to_continuous(df.c[i,]$radio_E,df.c[i,]$Q_E)
    df.c[i,]$Q_I <- map_radio_to_continuous(df.c[i,]$radio_I,df.c[i,]$Q_I)
    df.c[i,]$Q_O <- map_radio_to_continuous(df.c[i,]$radio_O,df.c[i,]$Q_O)
  }
}

# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
domains<- c('cracker', 'knife', 'lightbulb', 'strawberry')
syllogisms = c('AO2', 'EA3', 'IE1', 'OA1')
n_obj = 4
corrs = c()
if (exists('models')){remove(models)}
for (n_obj in 4:10){
  model.domains = data.frame()
  for (d in domains){
    model.all<-read.csv(paste(model.dir,d,'/00/csv/lis_N0_M0_tfbt',
                   d,'_qud1figFull_AIEOc4CAEP1_n',n_obj,
                   '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:9)]
    model.sub<-model.all[model.all$X..syll%in%syllogisms,]
    model.sub$domain <- d
    model.m<-melt(model.sub,id.vars=c('X..syll','domain'))
    model.domains<-rbind(model.domains, model.m)
  }
  #rename for merging
  names(model.domains)<-c('syll','domain','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}

models$syll <- factor(models$syll)
models$domain <- factor(models$domain)

cncl_labels = levels(models$conclusion)
```

# 1. Outline

The full Bayesian thing can take a variety of forms. To do this properly, we must consider the generative process behind the data. This requires taking a careful look at the dependent measure and how it can be related to probability distributions. It also requires careful consideration of the cogntive model, and especially, the *query statement*. 

## The cognitive model

Let's start with the cognitive model. Grossly, the model samples a situation from a binomial mixture and applies the quantifier premises to that situations. The model will reject the situation is it is inconsistent with the premises. If it is consistent with the premises, the model samples a conclusion which is true of the situation.

The model does this infinite times and returns a posterior distribution over conclusions (this is $P(conclusion | premises)$, or argument strength). 

## The experimental paradigm

For Experiment 1, I used 2 different dependent measures (let's call them R and S). For Experiment 2, I used only dependent measure R. Dependent Measure R was a 2AFC ("Follows"/"Doesn't follow") judgment accompanied with a confidence rating in the form of a slider bar (ranging form "Certain" to "Don't know"). Dependent Measure S was a combined judgment and confidence rating in the form of a slider bar ranging from "Certainly follows" to "Certainly doesn't follow". 

An analysis of R comparing the proportion of "Follows" judgments to the mean combined rating (radio + slider) revealed a strikingly high correlation (r>0.99), strongly suggesting no information gain from the confidence rating (see `belief-syll-exp.Rmd` for the analysis). Part of Experiment 1 was designed to look at the relationship between dependent measure R & S. The two dependent measures were highly, though not perfectly, correlated (r=0.91).

### Relating dependent measures to probability distributions

To do the full Bayesian thing, we must first imagine that data collected are samples from some probability distribution. 

#### Considering each conclusion independently.

**R**: if each conclusion is considered independently, then it is natural to think of a subject's response of a 2AFC as the result of a Bernoulli trial. 

Then, we can consider the responses of the experimental sample to be coming from a Binomial distribution, with success parameter $\theta_{ij}$, with $i \in \{all,some,none,not.all\}$ and $j \in \{syllogisms\}$. We can have at least two models of $\theta_{ij}$, corresponding to two different linking function assumptions.

1. $\theta_{ij}$ is $p_{ij}$. This is saying that responses are generated by flipping a coin and that coin weight is exactly the posterior probability of that conclusion (the output of the cognitive model). 
2. $\theta_{ij} \sim Beta$ with mean $= p_{ij}$ and some variance. This is saying that again the responses are generated by flipping a coin but that the coin weight is a random variable given by the mean of a Beta distribution. [Can we also get an estimate on the variance from the cognitive model? Perhaps by doing the bootstrap thing, or mcmc?].

```{r} 
estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}
```

##### Guessing

We will run into trouble with either of these approaches because there are conclusions with posterior probability = 1 (and others, = 0). This will yield poor estimates of the marginal probability of the data given the model. We may get around this problem in a number of ways:

A. The subjects' responses come from a mixture model. $\phi$ proportion of subjects' responses are imagined to be a result of random guessing. This will soak up some of the probabiltity mass and make non-zero outcomes to 0-probability conclusions at least somewhat plausible. 
B. The cognitive model might include some probability of random guessing, with the expression `(if (flip phi) true)` in the conditioning statement. I'm pretty sure this is equivalent to (1), though the interpretation of $phi$ is less transparent in this version.
C. Revise the cognitive model to take some number of samples. This will still have the problem of 0 probability events. 

Let's try to write down the model (2A).

```{r model2A}

setwd("/Users/mht/Documents/research/syllogism/analysis/")
models.c<-dcast(melt(models, id.vars=c('syll','domain','conclusion')), ... ~ conclusion)
for nobj in levels(models.c$variable){
  for dom in levels(models.c$domain){
    for sl in levels(models.c$syll){
      nobj = levels(models.c$variable)[1]
      dom = levels(models.c$domain)[1]
      sl = levels(models.c$syll)[1]
      model.preds = subset(models.c, domain==dom & variable==nobj)
      
cat('model{
  # cogntive model predictions, mu known
  lambda[j,k] ~ dgamma(.001,.001)
  sigma[j,k] <- 1/sqrt(lambda[j,k])
  alpha[j,k] <- (((1 - mu[j,k]) / (sigma[j,k]^2)) - (1 / mu[j,k])) * mu[j,k] ^ 2
  beta[j,k] <- alpha[j,k] * (1 / mu[j,k] - 1)
  phi[j,k] ~ dbeta(alpha[j,k],beta[j,k])


  # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
    x[i,j] ~ dbern(psi)
  # Guesses
  psi <- 0.5
  # base rate of guessing, for a particular syllogism
  psi[j] ~ dbeta(1,1)
  
  # Rate Of Success -- posterior of model
  
  # Data are Bernoulli With Rate Given either cogmod or guess
    theta[i,j,k] <- equals(x[i,j],0)*phi+equals(x[i,j],1)*0.5
    d[i,j,k] ~ dbin(theta[i,j,k],1)

}', file={f<-tempfile()})

tot.subj <- length(levels(factor(subset(df.c,condition=='radio')$subj)))
tot.syll <- length(levels(model.preds$syll)) #number of syllogisms
tot.resp <- length(levels(models$conclusion)) # number of responses


sapply(model.preds[1,cncl_labels], estBetaParams, var=0.01)


data <- list("tot.subj", "tot.syll", "tot.resp") # to be passed on to JAGS
myinits <-  list(
  list(phi = 0.25, z = round(runif(p)))) # Initial group assignment

# parameters to be monitored:  
parameters <- c("phi","z")

samples <- jags(data, inits=myinits, parameters,
   			 model.file =f, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)


    }
  }
}

