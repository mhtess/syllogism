---
title: "syll-tfbt"
author: "mht"
date: "October 31, 2014"
output: html_document
---

# The Full Bayesian Thing for syllogisms


```{r dataload, echo=FALSE}
library(R2jags)
library(gridExtra)

agr.ci.collapsed <- function(x){
  agr = aggregate(value ~ domain + syll + variable + experiment, data=x, FUN=mean)
  agr$CILow = aggregate(value ~ domain + syll + variable + experiment, data=x, FUN=ci.low)$value
  agr$CIHigh = aggregate(value ~ domain + syll + variable + experiment, data=x, FUN=ci.high)$value
  agr$YMin = agr$value - agr$CILow
  agr$YMax = agr$value + agr$CIHigh
  return(agr)
}
map_radio_to_continuous <- function(rad,cts){50+(2*rad-1)*(cts/2)}


exp_domains = c('_ crackers that have lots of flavor are soggy',
                '_ knives that cut well are sharp',
                '_ lightbulbs that are hot are on',
                '_ strawberries that are warm are in the freezer')
resp_labels<-c("Q_A","Q_E","Q_I","Q_O")
conclusion_labels<- c('all','none','some','some...not')
domains<- c('cracker', 'knife', 'lightbulb', 'strawberry')

# Load data from 2 experiments
fpath = '/Users/mht/Documents/research/syllogism/data/03syllogism_reasoning/'
fpath2 = '/Users/mht/Documents/research/syllogism/data/04syllogism_reasoning/'

df<-read.csv((paste(fpath,'syllbelief-exp-mturk_all_n250.csv',sep="")))
df$experiment <- factor(1)

df2<-read.csv((paste(fpath2,'syllbelief-exp2-mturk.csv',sep="")))
df2$condition<-paste(df2$condition,'2',sep='.')
df2$experiment <- factor(2)

exp1sylls<-levels(df$syll)
exp2sylls<-levels(df2$syll)


# remove extreme RTs
df<-subset(df,rt<mean(df$rt)+2*sqrt(var(df$rt)))
df2<-subset(df2,rt<mean(df2$rt)+2*sqrt(var(df2$rt)))

df2$subj<- factor(df2$subj, labels=as.integer(substring(levels(df2$subj),2,4))+length(levels(df$subj)))

df.c <- rbind(df,df2)
df.c$condition<-factor(df.c$condition,levels=c('slide','radio','radio.2'))

# map radio + slider to just radio
for (i in 1:length(df.c$subj)){
  if (substring(df.c[i,]$condition,1,5)=='radio'){
    df.c[i,]$Q_A <- map_radio_to_continuous(df.c[i,]$radio_A,df.c[i,]$Q_A)
    df.c[i,]$Q_E <- map_radio_to_continuous(df.c[i,]$radio_E,df.c[i,]$Q_E)
    df.c[i,]$Q_I <- map_radio_to_continuous(df.c[i,]$radio_I,df.c[i,]$Q_I)
    df.c[i,]$Q_O <- map_radio_to_continuous(df.c[i,]$radio_O,df.c[i,]$Q_O)
  }
}

df.norm = ddply(df.c, .(domain,condition,syll,experiment), 
  function(x) {resp = x[,resp_labels]
  resp / rowSums(resp)})

df.unnorm = ddply(df.c, .(domain,condition,syll,experiment), 
  function(x) {x[,resp_labels]})



```





# 0. Frequentist stuff

## $H_3$ = empirical prior (literal)

```{r h3exp1,echo=FALSE}

EP = 1
alt = 1
indevals = 1
mw = 0

#collapsed.bs <- subset(agr.ci.collapsed(melt(df.norm)),experiment==1)
#collapsed.bs$conclusion = factor(collapsed.bs$variable, labels=c('all.C.A','none.C.A','some.C.A','not.all.C.A'))
#collapsed.bs$domain = factor(collapsed.bs$domain, labels=domains)
if (indevals==1){
  collapsed.bs <- agr.ci.collapsed(melt(df.unnorm))
  } else {
  collapsed.bs <- agr.ci.collapsed(melt(df.norm))
}


collapsed.bs$conclusion <- factor(collapsed.bs$variable, labels=conclusion_labels)
collapsed.bs$domain <- factor(collapsed.bs$domain, labels=domains)


# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
syllogisms = c('AO2', 'EA3', 'IE1', 'OA1')
syllogisms = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1')

n_obj = 4
corrs = c()
if (exists('models')){remove(models)}

total_objs = seq(3,11,1)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (d in domains){
    if (indevals==1){
      model.tmp<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                   d,'_qud1MW',mw,'figFull_AIEOc4CAEP',EP,'Alt',alt,'Indevals',indevals,'_n',n_obj,
                   '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:12)]
      
      model.all <- with(model.tmp, data.frame(X..syll,
                                              all = all.C.A + all.some.C.A,
                                              none = none.C.A + none.not.all.C.A,
                                              some = some.C.A + all.some.C.A + some.not.all.C.A,
                                              some...not = not.all.C.A + none.not.all.C.A + some.not.all.C.A
                                              ))
    } else{
      model.all<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                                d,'_qud1figFull_AIEOc4CAEP',EP,'_n',n_obj,
                                '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:9)]
    }
    model.sub<-model.all[model.all$X..syll%in%syllogisms,]
    model.sub$domain <- d
    model.m<-melt(model.sub, id.vars=c('X..syll','domain'))
    model.domains<-rbind(model.domains, model.m)
  }
  
  #rename for merging
  names(model.domains)<-c('syll','domain','conclusion',paste('n',n_obj,sep=''))
  
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}



models$syll <- factor(models$syll)
models$domain <- factor(models$domain)
models$conclusion = factor(models$conclusion, labels = conclusion_labels)

m.models<-melt(models, id.vars=c('syll','domain','conclusion'))

all.stuff<-merge(m.models,collapsed.bs[c('domain','syll','value','conclusion')], 
      by=c('syll','domain','conclusion'))

model.fits<-ddply(all.stuff, .(variable), summarise, cor(value.x, value.y))
names(model.fits)<-c('n','correlation')
model.fits$n<-as.integer(substring(model.fits$n,2,3))
max.loc<-which.max(model.fits$correlation)

ggplot(model.fits, aes(x=n,y=correlation))+
  geom_bar(stat='identity')+#,fill='white')+
  geom_text(aes(x=n,y=0.3, label=round(correlation,2)),size=5)+
  scale_x_continuous(breaks=seq(3,11,1))+
  coord_cartesian(ylim=c(0,1)) + 
  scale_y_continuous(breaks=c(0,0.5,1))+
  xlab('\n n_objects')+
  ylab('correlation\n')

```

### faceted grids of model predictions

```{r}
models$syllogism <- factor(models$syll, levels = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1'),
                           labels=c("Some of the As are not Bs\n All of the Cs are Bs",
                                    "All of the Bs are As\n None of Bs are Cs",
                                    "None of the As are Bs\n Some of the Bs are Cs",
                                    "All of the As are Bs\n Some of the Bs are not Cs",
                                    "All of the As are Bs\n All of the Bs are Cs",
                                    "Some of the As are Bs\n All of the Bs are Cs",
                                    "All of the As are Bs\n None of the Bs are Cs",
                                    "Some of the As are Bs\n None of the Bs are Cs"
                           ))
                                    
models$domain <- factor(models$domain, labels = exp_domains)
models$conclusion<-factor(models$conclusion, labels = c('all','none','some','some...not'))

# Faceted grids of model predictions (Syll X Domain)

ggplot(subset(models,syll%in%c('AO2', 'EA3', 'IE1', 'OA1')),
              aes(x=conclusion,y=n7,fill=conclusion))+
  geom_bar(position=position_dodge(.6), 
               width = .6,
              stat='identity')+
  facet_grid(domain~syllogism)+
  guides(fill=F)+
  theme(
    axis.text.x=element_text(angle=90,hjust=1,vjust=.5),
    strip.text.x = element_text(),
    strip.text.y = element_text(angle=0)
  )+
  coord_cartesian(ylim=c(0, 0.7)) + 
  scale_y_continuous(breaks=c(0.25,0.5))+
  xlab("\nconclusion")+
  ylab('posterior probability\n')

#ggsave(filename = paste('literal_EP',EP,'_n7_exp1.png',sep=''),plot1, width=32, height=16)


ggplot(subset(models,syll%in%c('AA1','AI1','EA1','EI1')),
              aes(x=conclusion,y=n7,fill=conclusion))+
  geom_bar(position=position_dodge(.6), 
           width = .6,
           stat='identity')+
  facet_grid(domain~syllogism)+
  guides(fill=F)+
  theme(
    axis.text.x=element_text(angle=90,hjust=1,vjust=.5),
    strip.text.x = element_text(),
    strip.text.y = element_text(angle=0)
  )+
  coord_cartesian(ylim=c(0, 0.7)) + 
  scale_y_continuous(breaks=c(0.25,0.5))+
  xlab("\nconclusion")+
  ylab('posterior probability\n')

```


### $H_{4}$ = empirical prior with pragmatics

```{r h4exp1, echo=FALSE}
# Load model predictions, for different parameter (n_object) values               
# H4: n_objects, alpha (empirical prior + pragmatics)

model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
domains<- c('cracker', 'knife', 'lightbulb', 'strawberry')
syllogisms = c('AO2', 'EA3', 'IE1', 'OA1')
n_obj = 4
corrs = c()
if (exists('models')){remove(models)}

# this code relies upon the same parameter values being explored for all
# e.g. same range of alpha values for all levels of n_obj

total_objs = seq(3,9,2)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (d in domains){
    dom.path = paste(model.dir,d,'/10/csv/',sep='')
    model.files <- list.files(dom.path)
    model.files <- model.files[!(grepl("CLonly",model.files))]
    model.files <- model.files[!(grepl("1000k",model.files))]
    model.files <- model.files[grepl(paste('EP',EP,"_n",n_obj,'_base',sep=''),model.files)]
    for (m.file in model.files){
      model.all<-read.csv(paste(dom.path,m.file,sep=''))[c(1,6:9)]
      model.sub<-model.all[model.all$X..syll%in%syllogisms,]
      model.sub$domain <- as.factor(d)
      model.sub$alpha <- as.numeric(substring(strsplit(m.file,'alphQ')[[1]][2],1,3))
      model.m<-melt(model.sub,id.vars=c('X..syll','domain','alpha'))
      model.domains<-rbind(model.domains, model.m)
    }
  }
  #rename for merging
  print(n_obj)
  names(model.domains)<-c('syll','domain','alpha','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}

models$syll <- factor(models$syll)

m.models<-melt(models, id.vars=c('syll','domain','alpha','conclusion'))

all.stuff<-merge(m.models,collapsed.bs[c('domain','syll','value','conclusion')], 
      by=c('syll','domain','conclusion'))

model.fits<-ddply(all.stuff, .(alpha, variable), summarise, cor(value.x, value.y))
names(model.fits)<-c('alpha','n','correlation')
model.fits$n<-as.integer(substring(model.fits$n,2,3))
model.fits$alpha<-factor(model.fits$alpha)
max.loc<-which.max(model.fits$correlation)

ggplot(model.fits, aes(x=n,y=alpha))+
  geom_tile(aes(fill = correlation), colour = "white") + 
  geom_tile(data=model.fits[max.loc,], aes(x=n,y=alpha, fill=correlation),
            size=2,colour='black')+
  geom_text(data=model.fits[max.loc,], aes(x=n,y=alpha, label=round(correlation,2)),
            size=5,colour='black')+
  scale_fill_gradient(low = "white", high = "steelblue",limits=c(0.6,0.85))+
  scale_x_continuous(breaks=3:11)

```

## Experiment 2

```{r syllabels}
sylllabels = c('all / all', 'some / all', 'all / none', 'some / none')
```


### $H_3$ = empirical prior (literal)

```{r h3exp2, echo=FALSE}

collapsed.bs <- subset(agr.ci.collapsed(melt(df.norm)),experiment==2)
collapsed.bs$conclusion = factor(collapsed.bs$variable, labels=c('all.C.A','none.C.A','some.C.A','not.all.C.A'))
collapsed.bs$domain = factor(collapsed.bs$domain, labels=domains)


# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
syllogisms = c("AA1", "AI1", "EA1", "EI1")

if (exists('models')){remove(models)}


total_objs = seq(3,9,2)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (d in domains){
    model.all<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                   d,'_qud1figFull_AIEOc4CAEP',EP,'_n',n_obj,
                   '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:9)]
    model.sub<-model.all[model.all$X..syll%in%syllogisms,]
    model.sub$domain <- d
    model.m<-melt(model.sub,id.vars=c('X..syll','domain'))
    model.domains<-rbind(model.domains, model.m)
  }
  #rename for merging
  names(model.domains)<-c('syll','domain','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}

models$syll <- factor(models$syll)
models$domain <- factor(models$domain)

cncl_labels = levels(models$conclusion)

m.models<-melt(models, id.vars=c('syll','domain','conclusion'))

all.stuff<-merge(m.models,collapsed.bs[c('domain','syll','value','conclusion')], 
      by=c('syll','domain','conclusion'))

model.fits<-ddply(all.stuff, .(variable), summarise, cor(value.x, value.y))
names(model.fits)<-c('n','correlation')
model.fits$n<-as.integer(substring(model.fits$n,2,3))
max.loc<-which.max(model.fits$correlation)

ggplot(model.fits, aes(x=n,y=factor(1)))+
  geom_tile(aes(fill = correlation), colour = "white") + 
  geom_tile(data=model.fits[max.loc,], aes(x=n,y=1, fill=correlation),
            size=2,colour='black')+
  geom_text(aes(x=n,y=1, label=round(correlation,2)),size=5,colour='black')+
  scale_fill_gradient(low = "white", high = "steelblue",limits=c(0.8,0.95))+
  theme_bw()+
  scale_x_continuous(breaks=3:11)+
  scale_y_discrete()

models$syll <- factor(models$syll, labels = sylllabels)
models$domain <- factor(models$domain, labels = exp_domains)
models$conclusion<-factor(models$conclusion, labels = c('all','none','some','some...not'))

ggplot(subset(models), aes(x=conclusion,y=n7,fill=conclusion))+
  geom_bar(stat='identity')+
  facet_grid(domain~syll)+
  #theme_bw()+
  theme_blackDisplay()+
  ylim(0,0.7)+
  guides(fill=F)+
    theme(axis.text.x=element_text(
    angle=90,hjust=1,vjust=.5,colour='gray50'),
    strip.text.x = element_text(),
    strip.text.y = element_text(angle=0, size=20),
    title = element_text(size=30))+
  xlab('')+
  ylab('posterior probability\n')+
  ggtitle('q1 of the A B \nq2 of the B C\n q3 of the A C\n')

```

### $H_{4}$ = empirical prior with pragmatics

```{r h4exp2, echo=FALSE}
# Load model predictions, for different parameter (n_object) values               
# H4: n_objects, alpha (empirical prior + pragmatics)

model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
domains<- c('cracker', 'knife', 'lightbulb', 'strawberry')
syllogisms = c("AA1", "AI1", "EA1", "EI1")

if (exists('models')){remove(models)}

# this code relies upon the same parameter values being explored for all
# e.g. same range of alpha values for all levels of n_obj
total_objs = seq(3,9,2)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (d in domains){
    dom.path = paste(model.dir,d,'/10/csv/',sep='')
    model.files <- list.files(dom.path)
    model.files <- model.files[!(grepl("CLonly",model.files))]
    model.files <- model.files[!(grepl("1000k",model.files))]
    model.files <- model.files[grepl(paste('EP',EP,"_n",n_obj,'_base',sep=''),model.files)]
    for (m.file in model.files){
      model.all<-read.csv(paste(dom.path,m.file,sep=''))[c(1,6:9)]
      model.sub<-model.all[model.all$X..syll%in%syllogisms,]
      model.sub$domain <- as.factor(d)
      model.sub$alpha <- as.numeric(substring(strsplit(m.file,'alphQ')[[1]][2],1,3))
      model.m<-melt(model.sub,id.vars=c('X..syll','domain','alpha'))
      model.domains<-rbind(model.domains, model.m)
    }
  }
  #rename for merging
  print(n_obj)
  names(model.domains)<-c('syll','domain','alpha','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}

models$syll <- factor(models$syll)

m.models<-melt(models, id.vars=c('syll','domain','alpha','conclusion'))

all.stuff<-merge(m.models,collapsed.bs[c('domain','syll','value','conclusion')], 
      by=c('syll','domain','conclusion'))

model.fits<-ddply(all.stuff, .(alpha, variable), summarise, cor(value.x, value.y))
names(model.fits)<-c('alpha','n','correlation')
model.fits$n<-as.integer(substring(model.fits$n,2,3))
model.fits$alpha<-factor(model.fits$alpha)
max.loc<-which.max(model.fits$correlation)

ggplot(model.fits, aes(x=n,y=alpha))+
  geom_tile(aes(fill = correlation), colour = "white") + 
  geom_tile(data=model.fits[max.loc,], aes(x=n,y=alpha, fill=correlation),
            size=2,colour='black')+
  geom_text(data=model.fits[max.loc,], aes(x=n,y=alpha, label=round(correlation,2)),
            size=5,colour='black')+
  scale_fill_gradient(low = "white", high = "steelblue",limits=c(0.6,0.99))+
  theme_bw()+
  scale_x_continuous(breaks=3:11)


ggplot(subset(models, alpha==5.5), aes(x=conclusion,y=n7,fill=conclusion))+
  geom_bar(stat='identity')+
  facet_grid(domain~syll)+
  theme_bw()+
  ylim(0,1)
```

# 1. Outline

The full Bayesian thing can take a variety of forms. To do this properly, we must consider the generative process behind the data. This requires taking a careful look at the dependent measure and how it can be related to probability distributions. It also requires careful consideration of the cogntive model, and especially, the *query statement*. 

## The cognitive model

Let's start with the cognitive model. Grossly, the model samples a situation from a binomial mixture and applies the quantifier premises to that situations. The model will reject the situation is it is inconsistent with the premises. If it is consistent with the premises, the model samples a conclusion which is true of the situation.

The model does this infinite times and returns a posterior distribution over conclusions (this is $P(conclusion | premises)$, or argument strength). 

## The experimental paradigm

For Experiment 1, I used 2 different dependent measures (let's call them R and S). For Experiment 2, I used only dependent measure R. Dependent Measure R was a 2AFC ("Follows"/"Doesn't follow") judgment accompanied with a confidence rating in the form of a slider bar (ranging form "Certain" to "Don't know"). Dependent Measure S was a combined judgment and confidence rating in the form of a slider bar ranging from "Certainly follows" to "Certainly doesn't follow". 

An analysis of R comparing the proportion of "Follows" judgments to the mean combined rating (radio + slider) revealed a strikingly high correlation (r>0.99), strongly suggesting no information gain from the confidence rating (see `belief-syll-exp.Rmd` for the analysis). Part of Experiment 1 was designed to look at the relationship between dependent measure R & S. The two dependent measures were highly, though not perfectly, correlated (r=0.91).

### Relating dependent measures to probability distributions

To do the full Bayesian thing, we must first imagine that data collected are samples from some probability distribution. 

#### Considering each conclusion independently.

**R**: if each conclusion is considered independently, then it is natural to think of a subject's response of a 2AFC as the result of a Bernoulli trial. 

Then, we can consider the responses of the experimental sample to be coming from a Binomial distribution, with success parameter $\theta_{ij}$, with $i \in \{all,some,none,not.all\}$ and $j \in \{syllogisms\}$. We can have at least two models of $\theta_{ij}$, corresponding to two different linking function assumptions.

1. $\theta_{ij}$ is $p_{ij}$. This is saying that responses are generated by flipping a coin and that coin weight is exactly the posterior probability of that conclusion (the output of the cognitive model). 
2. $\theta_{ij} \sim Beta$ with mean $= p_{ij}$ and some variance. This is saying that again the responses are generated by flipping a coin but that the coin weight is a random variable given by the mean of a Beta distribution. [Can we also get an estimate on the variance from the cognitive model? Perhaps by doing the bootstrap thing, or mcmc?].

```{r} 
estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}
```

##### Guessing

We will run into trouble with either of these approaches because there are conclusions with posterior probability = 1 (and others, = 0). This will yield poor estimates of the marginal probability of the data given the model. We may get around this problem in a number of ways:

A. The subjects' responses come from a mixture model. $\phi$ proportion of subjects' responses are imagined to be a result of random guessing. This will soak up some of the probabiltity mass and make non-zero outcomes to 0-probability conclusions at least somewhat plausible. 
B. The cognitive model might include some probability of random guessing, with the expression `(if (flip phi) true)` in the conditioning statement. I'm pretty sure this is equivalent to (1), though the interpretation of $phi$ is less transparent in this version.
C. Revise the cognitive model to take some number of samples. This will still have the problem of 0 probability events. 

Let's try to write down the model (2A).

```{r model2A}
library(abind)
# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'

experiment='Both'

if (experiment==1){
  syllogisms = c('AO2', 'EA3', 'IE1', 'OA1')
  target.buttons = c('radio')}
if (experiment==2){
  syllogisms = c('AA1','AI1','EA1','EI1')
  target.buttons = c('radio.2')}
if (experiment=='Both'){
  syllogisms = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1')
  target.buttons = c('radio','radio.2')}

EP = 1
alt = 1
indevals = 1
mw = 0

# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'

if (exists('models')){remove(models)}

total_objs = seq(3,11,1)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (d in domains){
    if (indevals==1){
      model.tmp<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                   d,'_qud1MW',mw,'figFull_AIEOc4CAEP',EP,'Alt',alt,'Indevals',indevals,'_n',n_obj,
                   '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:12)]
      
      model.all <- with(model.tmp, data.frame(X..syll,
                                              all = all.C.A + all.some.C.A,
                                              none = none.C.A + none.not.all.C.A,
                                              some = some.C.A + all.some.C.A + some.not.all.C.A,
                                              some...not = not.all.C.A + none.not.all.C.A + some.not.all.C.A
                                              ))
    } else{
      model.all<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                                d,'_qud1figFull_AIEOc4CAEP',EP,'_n',n_obj,
                                '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:9)]
    }
    model.sub<-model.all[model.all$X..syll%in%syllogisms,]
    model.sub$domain <- d
    model.m<-melt(model.sub, id.vars=c('X..syll','domain'))
    model.domains<-rbind(model.domains, model.m)
  }
  
  #rename for merging
  names(model.domains)<-c('syll','domain','conclusion',paste('n',n_obj,sep=''))
  
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}



models$syll <- factor(models$syll)
models$domain <- factor(models$domain)

cncl_labels = levels(models$conclusion)

setwd("/Users/mht/Documents/research/syllogism/analysis/")
models.c<-dcast(melt(models, id.vars=c('syll','domain','conclusion')), ... ~ conclusion)

model.preds = subset(models.c)
data.obs = subset(df.c, (condition%in%target.buttons))[,c("subj","syll","domain","radio_A","radio_E","radio_I","radio_O")] # subset data condition==radio and take only radio button vals
data.obs = data.obs[order(data.obs[,1]),]
data.obs$subj<- factor(data.obs$subj)
data.obs$syll<- factor(data.obs$syll)
data.obs$domain<- factor(data.obs$domain)


tot.subj <- length(levels(data.obs$subj))
tot.syll <- length(levels(model.preds$syll)) #number of syllogisms
tot.resp <- length(levels(models$conclusion)) # number of responses
tot.dom <- length(levels(models$domain)) # number of domains
tot.n <- length(levels(model.preds$variable))

# # data is organized,  syll X response X domain X subj [i,j,u,k]
data.array<-abind(
      Map(
        function(x){          
          sb = subset(data.obs,subj==x)
          list.of.sb.arrays <- Map(function(d) {
            syll.data <- subset(sb,domain==d)
            a = syll.data$syll
            blank = array(NA,c(tot.syll,tot.resp))
            if (nrow(syll.data)!=0){
              j = Position(function(y){a==y},levels(data.obs$syll))
              blank[j,1:4] = syll.data[,c('radio_A','radio_E','radio_I','radio_O')]
              }
            b = matrix(blank,nrow=tot.syll)
            return(b)
          }, 
          levels(data.obs$domain))
          return(abind(list.of.sb.arrays,along=3))
          },
         levels(data.obs$subj)),along=4)

# model is organized,  syll X response X domain X subj [i,j,u,k]
model.array<-abind(
      Map(
        function(x){          
          sb = subset(model.preds,variable==x)
          list.of.sb.arrays <- Map(function(d) {
            syll.data <- subset(sb,domain==d)
            a = syll.data$syll
            blank = array(NA,c(tot.syll,tot.resp))
            j = match(levels(data.obs$syll),a)
            blank = syll.data[j,c('all','none','some','some...not')]
            b = array(blank)
            return(b)
          }, 
          levels(model.preds$domain))
          return(abind(list.of.sb.arrays,along=3))
          },
         levels(model.preds$variable)),along=4)

# data is organized,  syll X response X subj [i,j,k]
# data.array<-abind(
#       Map(
#         function(x){          
#           sb = subset(data.obs,subj==x)
#             a = sb$syll
#             blank = array(NA,c(4,4))
#             if (nrow(sb)!=0){
#               j = Position(function(y){a==y},levels(data.obs$syll))
#               blank[j,1:4] = sb[,c('radio_A','radio_E','radio_I','radio_O')]
#               }
#             b = matrix(blank,nrow=4)
#             return(b)
#           },
#          levels(data.obs$subj)),along=3)
# 
# 
# 
# # model to be organized,  syll X response X n_objects [i,j,k]
# model.array<-abind(
#       Map(
#         function(x){
#           sb = subset(model.preds,variable==x)
#           a = sb$syll
#           j = match(a,levels(model.preds$syll))
#           blank = array(NA,c(4,4))
#           blank = sb[j,c('all.C.A','none.C.A','some.C.A','not.all.C.A')]
#           b = array(blank)
#           return(b)
#           },
#          levels(model.preds$variable)),along=3)
# cat('model{
#   # cogntive model predictions
#   n ~ dcat(prior_on_objects[])
# 
#   # base rate of guessing, globally
#   phi  ~ dbeta(1,1)
#     for (i in 1:tot.syll){
#       for (j in 1:tot.resp){
#         for (k in 1:tot.subj){
#         # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
#         x[i,j,k] ~ dbern(phi)
#         # Data are Bernoulli With Rate Given either cogmod or guess
#         theta[i,j,k] <- equals(x[i,j,k],0)*churchmod[i,j,n]+equals(x[i,j,k],1)*0.5
#         d[i,j,k] ~ dbern(theta[i,j,k])
#   }}}
# 
# }', file={f<-tempfile()})


cat('model{
  # cogntive model predictions
  n ~ dcat(prior_on_objects[])

  # base rate of guessing, globally
  phi  ~ dbeta(1,1)
  for (u in 1:tot.dom){
    for (i in 1:tot.syll){
      for (j in 1:tot.resp){
        for (k in 1:tot.subj){
        # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
        x[i,j,u,k] ~ dbern(phi)
        # Data are Bernoulli With Rate Given either cogmod or guess
        theta[i,j,u,k] <- equals(x[i,j,u,k],0)*churchmod[i,j,u,n]+equals(x[i,j,u,k],1)*0.5
        d[i,j,u,k] ~ dbern(theta[i,j,u,k])
  }}}}

}', file={f<-tempfile()})

# cat('model{
#   # cogntive model predictions
#   n ~ dcat(prior_on_objects[])
# 
# 
# for (i in 1:tot.syll){
#   # base rate of guessing, by syll
#   phi[i]  ~ dbeta(1,1)
#   for (u in 1:tot.dom){
#     for (j in 1:tot.resp){
#         for (k in 1:tot.subj){
#         # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
#         x[i,j,u,k] ~ dbern(phi[i])
#         # Data are Bernoulli With Rate Given either cogmod or guess
#         theta[i,j,u,k] <- equals(x[i,j,u,k],0)*churchmod[i,j,u,n]+equals(x[i,j,u,k],1)*0.5
#         d[i,j,u,k] ~ dbern(theta[i,j,u,k])
#   }}}}
# 
# }', file={f<-tempfile()})

emprior <- 1
depth <- 0
prior_on_objects <- rep(1/tot.n,tot.n) # uniform prior on objects
d<- data.array
churchmod <-model.array
#sapply(model.preds[1,cncl_labels], estBetaParams, var=0.01)
# 
# data <- list("tot.subj", "tot.syll", "tot.resp",
#              "d","prior_on_objects","churchmod") # to be passed on to JAGS
# #initial values for guessing
# x.init = array(round(runif(tot.subj*tot.syll*tot.resp)),
#                  c(tot.syll,tot.resp,tot.subj))
# postz = which(churchmod==0,arr.ind=TRUE)
# for (pz in 1:length(postz[,1])){
#   x.init[postz[pz,c('dim1')],postz[pz,c('dim2')],]=1
# }
# x.init2 = array(round(runif(tot.subj*tot.syll*tot.resp)),
#                  c(tot.syll,tot.resp,tot.subj))
# for (pz in 1:length(postz[,1])){
#   x.init2[postz[pz,c('dim1')],postz[pz,c('dim2')],]=1
# }



data <- list("tot.subj", "tot.syll", "tot.resp","tot.dom",
             "d","prior_on_objects","churchmod") # to be passed on to JAGS
#initial values for guessing
x.init = array(round(runif(tot.subj*tot.syll*tot.resp*tot.dom)),
                 c(tot.syll,tot.resp,tot.dom,tot.subj))
x.init2 = array(round(runif(tot.subj*tot.syll*tot.resp*tot.dom)),
                 c(tot.syll,tot.resp,tot.dom,tot.subj))

postz = which(churchmod==0,arr.ind=TRUE)
for (pz in 1:length(postz[,1])){
  x.init[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=1
}

for (pz in 1:length(postz[,1])){
  x.init2[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=1
}

# postz = which(churchmod==1,arr.ind=TRUE)
# for (pz in 1:length(postz[,1])){
#   x.init[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=0
# }
# 
# for (pz in 1:length(postz[,1])){
#   x.init2[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=0
# }

myinits <-  list(
  list(phi = 0.5, 
       x = x.init,
      n = 4
       ),
    list(phi = 0.5, 
       x = x.init2,
      n = 4
       ))

# myinits <-  list(
#   list(
#        x = x.init,
#       n = 4
#        ),
#     list(
#        x = x.init2,
#       n = 4
#        ))



# parameters to be monitored:  
parameters <- c("n","phi")

samples <- jags(data, inits=myinits, parameters,
   			 model.file =f, n.chains=2, n.iter=1000, 
         n.burnin=10, n.thin=1, DIC=T)


df <-data.frame(n=samples$BUGSoutput$sims.list$n,
                phi=samples$BUGSoutput$sims.list$phi)



setwd("~/Documents/research/syllogism/presentations/labmtg-111914")

plot1<-ggplot(df, aes(x=2+n))+
  geom_histogram()+
  theme_solarized()+
  scale_x_continuous(breaks=seq(3,11))+
  guides(fill=F)+
  xlab("\n n_objects")

ggsave(filename = paste('literalEP',EP,'_posteriorN_exp',experiment,'.png',sep=''),plot1, width=16, height=12)



ggplot(df, aes(x=phi))+
  geom_density(fill='white')+
  theme_solarized()+
  xlab("\n phi")+
    guides(fill=F)+
    coord_cartesian(xlim=c(0, 1)) + 
  scale_x_continuous(breaks=c(0.25,0.5,0.75))

ggsave(filename = paste('literalEP',EP,'_posteriorPhi_exp',experiment,'retest.png',sep=''),plot2, width=16, height=12)


# df.m=melt(df[,2:9])
# df.m$syll<-factor(df.m$variable,labels=levels(data.obs$syll))

# df.m$syllogism <- factor(df.m$syll, levels = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1'),
#                            labels=c("Some of the As are not Bs\n All of the Cs are Bs",
#                                     "All of the Bs are As\n None of Bs are Cs",
#                                     "None of the As are Bs\n Some of the Bs are Cs",
#                                     "All of the As are Bs\n Some of the Bs are not Cs",
#                                     "All of the As are Bs\n All of the Bs are Cs",
#                                     "Some of the As are Bs\n All of the Bs are Cs",
#                                     "All of the As are Bs\n None of the Bs are Cs",
#                                     "Some of the As are Bs\n None of the Bs are Cs"
#                            ))
# 
# plot1<-ggplot(df.m, aes(x=value,fill=syllogism))+
#   geom_histogram()+
#   theme_blackDisplay()+
#   facet_wrap(~syllogism,nrow=2)+
#     coord_cartesian(xlim=c(0, 1)) + 
#   scale_x_continuous(breaks=c(0.5))+
#   guides(fill=F)+
#   xlab("\n phi")
# 
# ggsave(filename = paste('literalEP',EP,'_posteriorPhi_exp',experiment,'_facetSyll.png',sep=''),plot1, width=24, height=18)
# 

# plot2<-ggplot(df, aes(x=2+n))+
#   geom_histogram(fill='white')+
#   theme_blackDisplay()+
#   scale_x_continuous(breaks=seq(3,11))+
#   guides(fill=F)+
#   xlab("\n n_objects")
# 
# ggsave(filename = paste('literalEP',EP,'_posteriorN_exp',experiment,'_facetSyll.png',sep=''),plot2, width=16, height=12)







# more complex; not complete

# cat('model{
#   # cogntive model predictions, mu known, lambda maybe known
#   lambda[j,k] ~ dgamma(0.001,0.001)
#   sigma[j,k] <- 1/sqrt(lambda[j,k])
#   alpha[j,k] <- (((1 - mu[j,k]) / (sigma[j,k]^2)) - (1 / mu[j,k])) * mu[j,k] ^ 2
#   beta[j,k] <- alpha[j,k] * (1 / mu[j,k] - 1)
#   phi[j,k] ~ dbeta(alpha[j,k],beta[j,k])
# 
# 
#   # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
#     x[i,j] ~ dbern(psi)
#   # Guesses
#   psi <- 0.5
#   # base rate of guessing, for a particular syllogism
#   psi[j] ~ dbeta(1,1)
#   
#   # Rate Of Success -- posterior of model
#   
#   # Data are Bernoulli With Rate Given either cogmod or guess
#     theta[i,j,k] <- equals(x[i,j],0)*phi[j,k]+equals(x[i,j],1)*0.5
#     d[i,j,k] ~ dbern(theta[i,j,k])
# 
# }', file={f<-tempfile()})



```

Let's try analyzing the 4 domains independently, 


```{r model2A}
library(abind)
# Load model predictions, for different parameter (n_object) values
model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/LATTICE_4_tfbt/'
#syllogisms = c('AO2', 'EA3', 'IE1', 'OA1')
syllogisms = c('AA1','AI1','EA1','EI1')


corrs = c()
if (exists('models')){remove(models)}

df<-data.frame()
for (n_obj in 3:11){
  model.domains = data.frame()
  for (d in domains){
    model.all<-read.csv(paste(model.dir,'/00/csv/lis_N0_M0_tfbt',
                   d,'_qud1figFull_AIEOc4CAEP1_n',n_obj,
                   '_base0.00_s100k_alphQ1_alphR1_bsmean.csv',sep=''))[c(1,6:9)]
    model.sub<-model.all[model.all$X..syll%in%syllogisms,]
    model.sub$domain <- d
    model.m<-melt(model.sub,id.vars=c('X..syll','domain'))
    model.domains<-rbind(model.domains, model.m)
  }
  #rename for merging
  names(model.domains)<-c('syll','domain','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
  }
}

models$syll <- factor(models$syll)
models$domain <- factor(models$domain)

cncl_labels = levels(models$conclusion)

setwd("/Users/mht/Documents/research/syllogism/analysis/")
models.c<-dcast(melt(models, id.vars=c('syll','domain','conclusion')), ... ~ conclusion)

plurd<- levels(df.c$domain)
names(plurd)<-levels(models$domain)
for (dom.index in levels(models$domain)){

  model.preds = subset(models.c, domain==dom.index)
  data.obs = subset(df.c, (condition=='radio.2' & domain==plurd[[dom.index]]))[,c("subj","syll","domain","radio_A","radio_E","radio_I","radio_O")] # subset data condition==radio and take only radio button vals
  data.obs = data.obs[order(data.obs[,1]),]
  data.obs$subj<- factor(data.obs$subj)
  data.obs$syll<- factor(data.obs$syll)
  data.obs$domain<- factor(data.obs$domain)
  
  
  tot.subj <- length(levels(data.obs$subj))
  tot.syll <- length(levels(model.preds$syll)) #number of syllogisms
  tot.resp <- length(levels(models$conclusion)) # number of responses
  tot.dom <- length(levels(models$domain)) # number of domains
  tot.n <- length(levels(model.preds$variable))

# # data is organized,  syll X response X domain X subj [i,j,u,k]
# data.array<-abind(
#       Map(
#         function(x){          
#           sb = subset(data.obs,subj==x)
#           list.of.sb.arrays <- Map(function(d) {
#             syll.data <- subset(sb,domain==d)
#             a = syll.data$syll
#             blank = array(NA,c(4,4))
#             if (nrow(syll.data)!=0){
#               j = Position(function(y){a==y},levels(data.obs$syll))
#               blank[j,1:4] = syll.data[,c('radio_A','radio_E','radio_I','radio_O')]
#               }
#             b = matrix(blank,nrow=4)
#             return(b)
#           }, 
#           levels(data.obs$domain))
#           return(abind(list.of.sb.arrays,along=3))
#           },
#          levels(data.obs$subj)),along=4)
# 
# # model is organized,  syll X response X domain X subj [i,j,u,k]
# model.array<-abind(
#       Map(
#         function(x){          
#           sb = subset(model.preds,variable==x)
#           list.of.sb.arrays <- Map(function(d) {
#             syll.data <- subset(sb,domain==d)
#             a = syll.data$syll
#             blank = array(NA,c(4,4))
#             j = match(a,levels(model.preds$syll))
#             blank = syll.data[j,c('all.C.A','none.C.A','some.C.A','not.all.C.A')]
#             b = array(blank)
#             return(b)
#           }, 
#           levels(model.preds$domain))
#           return(abind(list.of.sb.arrays,along=3))
#           },
#          levels(model.preds$variable)),along=4)

  # data is organized,  syll X response X subj [i,j,k]
  data.array<-abind(
        Map(
          function(x){          
            sb = subset(data.obs,subj==x)
              a = sb$syll
              blank = array(NA,c(4,4))
              if (nrow(sb)!=0){
                j = Position(function(y){a==y},levels(data.obs$syll))
                blank[j,1:4] = sb[,c('radio_A','radio_E','radio_I','radio_O')]
                }
              b = matrix(blank,nrow=4)
              return(b)
            },
           levels(data.obs$subj)),along=3)



  # model to be organized,  syll X response X n_objects [i,j,k]
  model.array<-abind(
        Map(
          function(x){
            sb = subset(model.preds,variable==x)
            a = sb$syll
            j = match(a,levels(model.preds$syll))
            blank = array(NA,c(4,4))
            blank = sb[j,c('all.C.A','none.C.A','some.C.A','not.all.C.A')]
            b = array(blank)
            return(b)
            },
           levels(model.preds$variable)),along=3)

  cat('model{
    # cogntive model predictions
    n ~ dcat(prior_on_objects[])
  
    # base rate of guessing, globally
    phi  ~ dbeta(1,1)
      for (i in 1:tot.syll){
        for (j in 1:tot.resp){
          for (k in 1:tot.subj){
          # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
          x[i,j,k] ~ dbern(phi)
          # Data are Bernoulli With Rate Given either cogmod or guess
          theta[i,j,k] <- equals(x[i,j,k],0)*churchmod[i,j,n]+equals(x[i,j,k],1)*0.5
          d[i,j,k] ~ dbern(theta[i,j,k])
    }}}
  
  }', file={f<-tempfile()})


# cat('model{
#   # cogntive model predictions
#   n ~ dcat(prior_on_objects[])
# 
#   # base rate of guessing, globally
#   phi  ~ dbeta(1,1)
#   for (u in 1:tot.dom){
#     for (i in 1:tot.syll){
#       for (j in 1:tot.resp){
#         for (k in 1:tot.subj){
#         # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
#         x[i,j,u,k] ~ dbern(phi)
#         # Data are Bernoulli With Rate Given either cogmod or guess
#         theta[i,j,u,k] <- equals(x[i,j,u,k],0)*churchmod[i,j,u,n]+equals(x[i,j,u,k],1)*0.5
#         d[i,j,u,k] ~ dbern(theta[i,j,u,k])
#   }}}}
# 
# }', file={f<-tempfile()})

  emprior <- 1
  depth <- 0
  prior_on_objects <- rep(1/tot.obj,tot.obj) # uniform prior on objects
  d<- data.array
  churchmod <- model.array
#sapply(model.preds[1,cncl_labels], estBetaParams, var=0.01)

  data <- list("tot.subj", "tot.syll", "tot.resp",
               "d","prior_on_objects","churchmod") # to be passed on to JAGS
  #initial values for guessing
  x.init = array(round(runif(tot.subj*tot.syll*tot.resp)),
                   c(tot.syll,tot.resp,tot.subj))
  postz = which(churchmod==0,arr.ind=TRUE)
  for (pz in 1:length(postz[,1])){
    x.init[postz[pz,c('dim1')],postz[pz,c('dim2')],]=1
  }
  x.init2 = array(round(runif(tot.subj*tot.syll*tot.resp)),
                   c(tot.syll,tot.resp,tot.subj))
  for (pz in 1:length(postz[,1])){
    x.init2[postz[pz,c('dim1')],postz[pz,c('dim2')],]=1
  }



# data <- list("tot.subj", "tot.syll", "tot.resp","tot.dom",
#              "d","prior_on_objects","churchmod") # to be passed on to JAGS
# #initial values for guessing
# x.init = array(round(runif(tot.subj*tot.syll*tot.resp*tot.dom)),
#                  c(tot.syll,tot.resp,tot.dom,tot.subj))
# 
# postz = which(churchmod==0,arr.ind=TRUE)
# for (pz in 1:length(postz[,1])){
#   x.init[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=1
# }
# 
# x.init2 = array(round(runif(tot.subj*tot.syll*tot.resp*tot.dom)),
#                  c(tot.syll,tot.resp,tot.dom,tot.subj))
# for (pz in 1:length(postz[,1])){
#   x.init2[postz[pz,c('dim1')],postz[pz,c('dim2')],postz[pz,c('dim3')],]=1
# }

  myinits <-  list(
    list(phi = 0.1, 
         x = x.init,
        n = 1
         ),
      list(phi = 0.2, 
         x = x.init2,
        n = 4
         ))
  
  # parameters to be monitored:  
  parameters <- c("n","phi")

  samples <- jags(data, inits=myinits, parameters,
       		 model.file =f, n.chains=2, n.iter=1000, 
           n.burnin=100, n.thin=1, DIC=T)


df.temp <-data.frame(n=samples$BUGSoutput$sims.list$n,
                phi=samples$BUGSoutput$sims.list$phi,
                domain=dom.index)

df<-rbind(df,df.temp)
}

# PLOT

setwd("~/Documents/research/syllogism/presentations/labmtg-111914")

plot1<-ggplot(df, aes(x=2+n,fill=domain))+
  geom_histogram()+
  theme_blackDisplay()+
  facet_wrap(~domain)+
  scale_x_continuous(breaks=seq(3,11))+
  guides(fill=F)+
  xlab("\n n_objects")

ggsave(filename = paste('literalEP',EP,'_posteriorN_exp2_facetDomain.png',sep=''),plot1, width=16, height=12)



plot2<-ggplot(df, aes(x=phi,fill=domain))+
  geom_density()+
  theme_blackDisplay()+
  facet_wrap(~domain)+
  xlab("\n phi")+
    guides(fill=F)+
    coord_cartesian(xlim=c(0, 1)) + 
  scale_x_continuous(breaks=c(0.25,0.5,0.75))


ggsave(filename = paste('literalEP',EP,'_posteriorPhi_exp2_facetDomain.png',sep=''),plot2, width=16, height=12)


```



# TFBT on Prior elicitation data

We generalize the "world bound" bayesian model (reasoning over background knowledge) to allow variability in background knowledge. This may be particularly important for beilef bias studies, where syllogistic premises (or conclusions) are often taken to contradict background knowledge (e.g. "Some robins are not birds""). This allows for the possibility that subjects reinterpret the premimse to be more consistent background knowledge (e.g. "Some batman sidekicks are not birds"). By putting a distribution over background knowledge, premises and knowledge are mutually constraining.

Previously, we have taken the mean noramlized endorsements as the parameters to the multinomial distribution from which states are sampled. Now, we will say that for each syllogism, a multinomial is sampled from a dirichlet distribution. The parameters of that dirichlet will be similar to the mean noramlized endorsement that we used previously. Sampling from this dirichlet will give the model flexibility to adjust the prior over states when the syllogistic premises are discordant with "the normal meaning of words" (which we might take the parameters of the dirichlet --- something like the mean normalized endorsements --- to represent). 

So our goal with this bayesian analysis of the prior elicitation data is to infer the hyperparameters of the dirichlet. The dirichlet can be parameterized by a multinomial distribution and a concentration parameter. The multinomial distribution will be something like the mean normalized endorsements, that we used in the simpler version of the "world bound" bayesian. The concentration parameter will be something like the consistency or agreement of subjects' endorsements; the higher the concentration parameter, the more likely the sampled multinomial will similar to the mean normalized distribution.


```{r model2A}
library(abind)

fpath = '/Users/mht/Documents/research/syllogism/data/03syllogism_prior_psychjs/'

df<-read.csv((paste(fpath,'prior-exp-mturk_all_n71.csv',sep="")))[c(1:12,16)]
df$condition<-factor(df$condition,labels=c('plausibility','frequency'))

df_norm = ddply(df, .(domain, condition,subj), function(x) {
  means = x[,c('Q_XYZ','Q_XYnZ','Q_XnYZ','Q_nXYZ','Q_XnYnZ','Q_nXYnZ','Q_nXnYZ','Q_nXnYnZ')]
  means / rowSums(means)
})
#write.csv(x=df_norm, file='prior-exp-mturk_all_n71_normed.csv')

# data.obs<-df_norm
data.obs<-melt(df_norm, id.vars =c("domain","condition","subj"))

setwd("/Users/mht/Documents/research/syllogism/analysis/")



tot.subj <- length(levels(data.obs$subj))
tot.dom <- length(levels(data.obs$domain)) #number of syllogisms
tot.cond <- length(levels(data.obs$condition)) # number of conditions
tot.resp <- length(levels(data.obs$variable)) # number of responses

# # organize data,  domain X response X subj [i,j,k]

data.array <- xtabs(value~.,subset(data.obs, condition=='frequency' & domain=='tomatoplant')[,c("variable","subj","value")])

cat('model{
alpha ~ dexp(1)
beta ~ ddirch(ones)
for (k in tot.subj){
xi[,k] ~ ddirch(alpha*beta)
}
}', file={f<-tempfile()})

# cat('model{
# for (k in tot.subj){
#   xi[,k] ~ ddirch(alphas[])
#   for(j in 1:tot.resp){
#     alphas[j] ~ dunif(0,20)
#   }
# }
# }', file={f<-tempfile()})


xi<- data.array
ones <- rep(1,tot.resp)
data <- list("tot.subj","xi","ones") # to be passed on to JAGS
myinits <-  list(list(alpha = 1, beta=ones))
# parameters to be monitored:  
parameters <- c("alpha","beta")

samples <- jags(data, inits=myinits, parameters,
     		 model.file =f, n.chains=1, n.iter=100000, 
         n.burnin=10, n.thin=100, DIC=T)


df <-data.frame(alpha= samples$BUGSoutput$sims.list$alpha,
                beta= samples$BUGSoutput$sims.list$beta)

ggplot(melt(df[,c(2:9)]), aes(x=value,fill=variable))+
  geom_histogram()+
  facet_wrap(~variable,nrow=2)+
  guides(fill=F)

ggplot(df,aes(x=alpha))+
  geom_histogram()

posterior.means<-ddply(melt(df[,c(2:9)]),.(variable),function(x)(median(x$value)))



posterior.means$normalized <- posterior.means$V1/sum(posterior.means$V1)
ggplot(posterior.means, aes(x=variable,y=normalized))+
  geom_bar(stat='identity')




posterior.means<-ddply(melt(df),.(variable),function(x)(mean(x$value)))
posterior.means$normalized <- posterior.means$V1/sum(posterior.means$V1)
ggplot(posterior.means, aes(x=variable,y=normalized))+
  geom_bar(stat='identity')








# data.array<- abind(Map(
#   function(x){
#   sb = subset(data.obs,subj==x)
#   a = sb$domain
#   blank = array(NA,c(tot.dom,tot.resp))
#   if (nrow(sb)!=0){
#   j = match(levels(data.obs$domain),a)
#   blank = sb[j,c('Q_XYZ','Q_XYnZ','Q_XnYZ','Q_nXYZ','Q_XnYnZ','Q_nXYnZ','Q_nXnYZ','Q_nXnYnZ')]
#   }
#   return(blank)
#   },
#   levels(data.obs$subj)),along=3)


# cat('model{
# for (k in tot.subj){
#   for(j in 1:tot.resp) {
#     xi_raw[j,k] ~ dgamma(alpha[j], 1)
#   }
# #  for(j in 1:tot.resp) {
# #    xi[j,k] <- xi_raw[j,k] / sum(xi_raw[,k])
# #  }
#   ## Some prior for alpha ...
#   for(j in 1:tot.resp){
#     alpha[j] ~ dunif(0,20)
#   }
# }
# }', file={f<-tempfile()})
#    



ggsave(filename = paste('literalEP',EP,'_posteriorN_exp',experiment,'.png',sep=''),plot1, width=16, height=12)



ggplot(df, aes(x=phi))+
  geom_density(fill='white')+
  theme_solarized()+
  xlab("\n phi")+
    guides(fill=F)+
    coord_cartesian(xlim=c(0, 1)) + 
  scale_x_continuous(breaks=c(0.25,0.5,0.75))

ggsave(filename = paste('literalEP',EP,'_posteriorPhi_exp',experiment,'retest.png',sep=''),plot2, width=16, height=12)


# df.m=melt(df[,2:9])
# df.m$syll<-factor(df.m$variable,labels=levels(data.obs$syll))

# df.m$syllogism <- factor(df.m$syll, levels = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1'),
#                            labels=c("Some of the As are not Bs\n All of the Cs are Bs",
#                                     "All of the Bs are As\n None of Bs are Cs",
#                                     "None of the As are Bs\n Some of the Bs are Cs",
#                                     "All of the As are Bs\n Some of the Bs are not Cs",
#                                     "All of the As are Bs\n All of the Bs are Cs",
#                                     "Some of the As are Bs\n All of the Bs are Cs",
#                                     "All of the As are Bs\n None of the Bs are Cs",
#                                     "Some of the As are Bs\n None of the Bs are Cs"
#                            ))
# 
# plot1<-ggplot(df.m, aes(x=value,fill=syllogism))+
#   geom_histogram()+
#   theme_blackDisplay()+
#   facet_wrap(~syllogism,nrow=2)+
#     coord_cartesian(xlim=c(0, 1)) + 
#   scale_x_continuous(breaks=c(0.5))+
#   guides(fill=F)+
#   xlab("\n phi")
# 
# ggsave(filename = paste('literalEP',EP,'_posteriorPhi_exp',experiment,'_facetSyll.png',sep=''),plot1, width=24, height=18)
# 

# plot2<-ggplot(df, aes(x=2+n))+
#   geom_histogram(fill='white')+
#   theme_blackDisplay()+
#   scale_x_continuous(breaks=seq(3,11))+
#   guides(fill=F)+
#   xlab("\n n_objects")
# 
# ggsave(filename = paste('literalEP',EP,'_posteriorN_exp',experiment,'_facetSyll.png',sep=''),plot2, width=16, height=12)







# more complex; not complete

# cat('model{
#   # cogntive model predictions, mu known, lambda maybe known
#   lambda[j,k] ~ dgamma(0.001,0.001)
#   sigma[j,k] <- 1/sqrt(lambda[j,k])
#   alpha[j,k] <- (((1 - mu[j,k]) / (sigma[j,k]^2)) - (1 / mu[j,k])) * mu[j,k] ^ 2
#   beta[j,k] <- alpha[j,k] * (1 / mu[j,k] - 1)
#   phi[j,k] ~ dbeta(alpha[j,k],beta[j,k])
# 
# 
#   # Each response Belongs To One Of Two Latent Groups (reasoning or guessing)
#     x[i,j] ~ dbern(psi)
#   # Guesses
#   psi <- 0.5
#   # base rate of guessing, for a particular syllogism
#   psi[j] ~ dbeta(1,1)
#   
#   # Rate Of Success -- posterior of model
#   
#   # Data are Bernoulli With Rate Given either cogmod or guess
#     theta[i,j,k] <- equals(x[i,j],0)*phi[j,k]+equals(x[i,j],1)*0.5
#     d[i,j,k] ~ dbern(theta[i,j,k])
# 
# }', file={f<-tempfile()})

```

## TFBT on prior elicitation



```{r}
library(polspline)
setwd("~/Documents/research/syllogism/analysis")

domain='lightbulb'
church.post<-read.csv('post_alpha-beta_mh1000_100_f1_lightbulb.csv',header=F)
#church.post<-read.csv(paste('post_alpha-beta_mh10000_100_f3_',domain,'.csv',sep=''),header=F)
names(church.post)<-c("alpha",names(df_norm)[4:11])

plabels1<- c("Q_nXnYnZ","Q_XnYnZ",'Q_nXYnZ','Q_nXnYZ',"Q_XYnZ","Q_XnYZ","Q_nXYZ","Q_XYZ")
plabels2<- c("000","100","010","001",'110','101','011','111')

post.m<-melt(church.post[,c(2:9)])

post.meds<-ddply(post.m,.(variable),summarise,
  med = median(value),
  mn = mean(value))


post.meds$variable<-factor(post.meds$variable, 
                           levels=plabels1, labels=plabels2)
post.m$variable<-factor(post.m$variable, levels=plabels1, labels=plabels2)

a<-ggplot(data=post.m,aes(x=value,fill=variable))+
  geom_density(alpha=0.8)+
  geom_vline(aes(xintercept=med), data=post.meds) +
  facet_wrap(~variable,nrow=1)+
  guides(fill=F)

ggsave(a,file=paste('posterior_beta_',domain,'_mh1000_100_f1.png',sep=''), width=18, height=4)

b<-ggplot(data=church.post,aes(x=alpha))+
  geom_density(alpha=0.7,fill='white')

ggsave(b,file=paste('posterior_alpha_',domain,'_mh1000_100_f1.png',sep=''),width=5, height=5)
)

```


### Done better, faster

```{r}
setwd("~/Documents/research/syllogism/analysis")
domain='lightbulb'
mhsamples='100k'
drift = '100000'
wp1<-read.csv(paste(domain,'posterior',mhsamples,'_drift',drift,'.csv',sep=''),header=T)
# wp1$alpha<-factor(round(wp1$alpha,3))
# wp1$d1<-factor(round(wp1$d1,3))
# wp1$d2<-factor(round(wp1$d2,3))
# wp1$d3<-factor(round(wp1$d3,3))
# wp1$d4<-factor(round(wp1$d4,3))
# wp1$d5<-factor(round(wp1$d5,3))
# wp1$d6<-factor(round(wp1$d6,3))
# wp1$d7<-factor(round(wp1$d7,3))
# wp1$d8<-factor(round(wp1$d8,3))
wp1$alpha<-factor(wp1$alpha)
wp1$d1<-factor(wp1$d1)
wp1$d2<-factor(wp1$d2)
wp1$d3<-factor(wp1$d3)
wp1$d4<-factor(wp1$d4)
wp1$d5<-factor(wp1$d5)
wp1$d6<-factor(wp1$d6)
wp1$d7<-factor(wp1$d7)
wp1$d8<-factor(wp1$d8)

a<-ddply(wp1, .(alpha), summarise, prob=sum(prob), variable='alpha')
names(a)[1] = 'value'
d1 = ddply(wp1, .(d1), summarise, prob=sum(prob), variable ='d1')
d2 = ddply(wp1, .(d2), summarise, prob=sum(prob), variable ='d2')
d3 = ddply(wp1, .(d3), summarise, prob=sum(prob), variable ='d3')
d4 = ddply(wp1, .(d4), summarise, prob=sum(prob), variable ='d4')
d5 = ddply(wp1, .(d5), summarise, prob=sum(prob), variable ='d5')
d6 = ddply(wp1, .(d6), summarise, prob=sum(prob), variable ='d6')
d7 = ddply(wp1, .(d7), summarise, prob=sum(prob), variable ='d7')
d8 = ddply(wp1, .(d8), summarise, prob=sum(prob), variable ='d8')
names(d1)[1] = 'value'
names(d2)[1] = 'value'
names(d3)[1] = 'value'
names(d4)[1] = 'value'
names(d5)[1] = 'value'
names(d6)[1] = 'value'
names(d7)[1] = 'value'
names(d8)[1] = 'value'
wp.marg<-rbind(a,d1,d2,d3,d4,d5,d6,d7,d8)
wp.marg$value<-to.n(wp.marg$value)
plevels<- c('alpha','d8','d5','d6','d7','d2','d3','d4','d1')
plabels<- c('alpha',"000","100","010","001",'110','101','011','111')

wp.marg$variable<-factor(wp.marg$variable, levels=plevels,labels=plabels)

wp.maxmarg<- ddply(wp.marg, .(variable), summarise, 
                   max.val= value[which.max(prob)],
                   expect = sum(value*prob),
                   max.prob = max(prob))

ggplot(subset(wp.marg,variable=='alpha'),aes(x=value,y=prob))+
  geom_bar(stat='identity',color='white')+
#  facet_wrap(~variable,nrow=1)+
  guides(fill=F)

plt<-ggplot(subset(wp.marg,variable!='alpha'),aes(x=value,y=prob,fill=variable,color=variable))+
  geom_bar(stat='identity')+
  facet_wrap(~variable,nrow=1)+
  guides(fill=F,color=F)


ggsave(plt,file=paste('posterior_betas_mh100k_drift',drift,'.png',sep=''),width=20,height=4)
# write.csv(x=t(wp.maxmarg), 
#           file=paste('max_posterior_priors', domain,'_mh',mhsamples,
#                      '.csv',sep=''))

# 
# 
# 
# wp.hist<-data.frame(value = unlist(sapply(1:nrow(wp.marg), function(i) {rep(wp.marg$value[i], round(wp.marg$prob[i]*10000))})),
# variable = unlist(sapply(1:nrow(wp.marg), function(i) {rep(as.character(wp.marg$variable[i]), round(wp.marg$prob[i]*10000))})))
# 
# 
# wp.hist$variable<-factor(wp.hist$variable, levels=plevels,labels=plabels)
# 
# 
# # ggplot(subset(wp.hist,variable=='alpha'),aes(x=value))+
# #   geom_histogram()
# # 
# ggplot(subset(wp.hist,variable!='alpha'),aes(x=value,group=variable,fill=variable))+
#   geom_histogram()+
#   facet_wrap(~variable,nrow=1)+
#   guides(fill=F)





# ggplot(subset(wp.marg,variable=='alpha'),aes(x=value,y=prob))+
#   geom_bar(stat='identity')


```


# Model with priors on priors

Using posterior medians for the lightbulb domain properties + alpha as dirichlet parameters, I sample 100 multinomials to serve as a hyperprior over state-priors for the syllogism model. The syllmodel then has uncertainty about what the world is like, and takes this into account when analyzing syllogisms.

Here are the results from the lightbulb domain, for n_objects =5, alpha =3.

```{r}
setwd("~/Documents/research/syllogism/models/modeldata/priors_on_priors")
lb<-read.csv('syllmodel_lightbulb_hyperprior100_n5_alpha3.csv', header=F)
```

### Priors-on-priors (POP) models

+ 1 -- dirichlet prior around posterior median dirichlet parameters (both alpha and betas, inferred from empirical prior data)
+ 2 -- `(dirichlet '(1 1 ... 1))` + `(multinomial empirical-prior-posterior-medians)`
+ 3 -- 1 but with a distribution over alpha ("How much do I trust this empirical prior?")

```{r pop.models}

model.dir<-'/Users/mht/Documents/research/syllogism/models/modeldata/priors_on_priors/'
syllogisms = c('AO2', 'EA3', 'IE1', 'OA1','AA1','AI1','EA1','EI1')
domains = c("cracker",'lightbulb','knife','strawberry')
if (exists('models')){remove(models)}

total_objs = seq(3,5,1)
total_alphas = seq(1.0,5.5,0.5)
options(digits=2)
for (n_obj in total_objs){
  model.domains = data.frame()
  for (alpha in total_alphas){
  for (d in domains){
    model.all<-read.csv(paste(model.dir,'syllmodel_',d,'_n',n_obj,'_alpha',sprintf('%.1f',alpha),'pop1_dirsamp100.csv',sep=''),
                        header=F,col.names=c("syll","all","none","some","notall"))
                              
    model.sub<-model.all[model.all$syll%in%syllogisms,]
    model.sub$domain <- d
    model.sub$alpha <- alpha
    model.m<-melt(model.sub,id.vars=c('syll','domain','alpha'))
    model.domains<-rbind(model.domains, model.m)
  }
}
  #rename for merging
  names(model.domains)<-c('syll','domain','alpha','conclusion',paste('n',n_obj,sep=''))
  if (exists('models')){
    models = merge(models,model.domains)
  } else {
    models = model.domains
}
}

models$syll <- factor(models$syll)
models$domain <- factor(models$domain)
models$conclusion <- factor(models$conclusion)
models$alpha <- factor(models$alpha)

m.models<-melt(models, id.vars=c('syll','domain','conclusion','alpha'))

```


Still need to load experimental data.
```{r}
all.stuff<-merge(m.models,
                 collapsed.bs[c('domain','syll','value','conclusion')], 
                 by=c('syll','domain','conclusion'))

model.fits<-ddply(all.stuff, .(variable,alpha), summarise, cor(value.x, value.y))

names(model.fits)<-c('n','alpha','correlation')

model.fits$n<-as.integer(substring(model.fits$n,2,3))

max.loc<-which.max(model.fits$correlation)




```


